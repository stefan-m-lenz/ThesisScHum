\title{Distributed deep learning on biomedical data with Boltzmann machines}
\author{Stefan Lenz}
\date{\today}

\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage[toc,page,title]{appendix}
\usepackage[times,inconsolata]{Rd}
\usepackage[parfill]{parskip}
\bibliographystyle{agsm}
\usepackage{amsmath,amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[nobiblatex]{xurl}
\usepackage{epstopdf}
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{placeins}
\usepackage{setspace}
\usepackage{lscape}
\usepackage[a4paper,%bindingoffset=0.2in,
            left=2.5cm,right=2.5cm,top=2.1cm,bottom=2.5cm]{geometry}
\usepackage{helvet}
\usepackage{pdfpages}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{titlesec}
\usepackage{rotating}
\setcounter{secnumdepth}{4}
\usepackage{afterpage}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\newcommand{\inlinecode}[1]{\texttt{#1}}
\newcommand{\sigm}{\mathrm{sigm}}
\newcommand{\ELBO}{\mathrm{ELBO}}
\newcommand{\apkg}[1]{\emph{#1}}
\newcommand{\proglang}[1]{#1} % egal, kommt in kopierten referenzen vor

\newcommand{\rightpageref}[1]{\hfill $\rightsquigarrow$ p. \pageref{#1}}
\interfootnotelinepenalty=10000

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}

\lstdefinelanguage{Julia}
  {morekeywords={begin,abstract,break,catch,const,continue,do,else,elseif,
      end,export,false,finally,for,function,global,if,import,let,local,
      macro,module,quote,return,struct,true,try,type,mutable,primitive,type
      using,while},
   sensitive=true,
   alsoother={$},
   morecomment=[l]\#,
   morestring=[s]{"}{"},
   morestring=[m]{'}{'},
}[keywords,comments,strings]

\lstset{
    language         = Julia,
    basicstyle       = \ttfamily,
    keywordstyle     = \bfseries,
    commentstyle = \itshape,
    showstringspaces = false,
    frame = single
}

\begin{document}
\includepdf[pages=-]{includes/Deckblatt}

%\maketitle

\tableofcontents
\newpage
\onehalfspacing

\section*{Abstract}
TODO
%We present the Julia package ``BoltzmannMachines.jl" that provides a friendly interface to algorithms for training and evaluating Boltzmann machines.
%The ability of Boltzmann machines to deal with rather small sample sizes makes them especially interesting for analyzing medical data sets. But since these data sets can be very heterogeneous compared to the widely used image data, the choice of hyperparameters for training is especially challenging.
%Therefore the package puts a strong focus on monitoring and evaluating the learning process.
%Primary evaluation criterion is the likelihood of the model, which can be estimated using annealed importance sampling (AIS). We present our approach for adapting the AIS algorithm on multimodal deep Boltzmann machines in detail in the article.
%Additionally to likelihood estimation, this package offers convenience methods to monitor a number of other statistics and it allows to easily extend the monitoring of the training.

\clearpage
\section{Introduction}
TODO privacy motivation for using generative models for synthetic data

TODO several approaches, e.g. synthpop. deep learning models as generative models capture more complicated structure

TODO pattern recognition

TODO deep learning popular with images, most popular use cases deal with large sample size

TODO DBMs especially interesting for biomedical data because small sample size

TODO GAN VAE

Saving parameters can aid in training models also with fewer data points as the performance of more complex models may deteriorate if the sample size is too small \citep{chan_classifier_1999}.
The problem of small sample sizes is particularly prevalent when dealing with genetic variant data 
because there is a high number of genetic variants, and additionally the sequencing of genomes is still relatively expensive.
This results in relatively few samples compared to the number of dimensions in data sets of genetic variants.

%Multimodal DBMs have been used by Srivastava et al. for image captions \citep{srivastava2012multimodal}.
%Here we want to broaden the scope a little bit and present an extensible approach for creating deep Boltzmann architectures with different types of visible units and flexible partitioned hidden layers. Training of such architectures ....
%Those architectures are not only useful for putting data of different types in one model but also in cases where a natural partitioning of the data can be derived from domain knowledge. With partitioning one can greatly reduce the number of parameters in the model. This allows training models on data sets where the sample size would otherwise be too small to fit a full model.


\clearpage
\section{Deep Boltzmann machines}
\subsection{Theory}

Boltzmann machines \citep{ackley_boltzmann_1985} are a special kind of neural networks.
They have been derived from models in statistical physics.
There, Ising models aim at modeling the magnetic spin in lattices of atoms or molecules in ferromagnetic crystals \citep{isingmodel}.
In Ising models models the probability of the magnetic spin of molecules in a lattice of molecules is determined by an energy function that assigns an energy to each possible configuration of magnetic spins in the lattice.
Configurations with a lower energy have a higher probability of occurring.
This connection between energy and and probability holds also for Boltzmann machines.
It makes Boltzmann machines a type of so-called ``energy-based" models \citep{ranzato_ebm}.

The analogy of lattices of molecules with magnetic spin are networks of neurons, where each cell may have a neural activation.
The analogy goes further.
Neural networks receive input from data in special nodes.
The information about the data can then be learned by the network.
In the physics analogue, this corresponds to parts of the lattice that are influenced by an external magnetic field.
In both cases, the information about the input is encapsulated in the parameters of the model, which influence the energy function and thereby determine the probability of the activations in the network.
With the information contained in the model parameters, a trained Boltzmann machine is then able to draw new samples according to the distribution that has been learned from the original data.
With this ability, Boltzmann machines can be employed as ``generative models".

The following parts of this section give an overview of the mathematical definitions surrounding Boltzmann machines that will be necessary to understand how Boltzmann machines can be trained and used.
This also includes the definition of some terminology that is commonly used in the literature about Boltzmann machines.

\subsubsection{Definition of Boltzmann machines}\label{basicbmproperties}

A Boltzmann machine model with parameters $\Theta$ defines a probability distribution $p(x)$ for a random variable $x = (v, h)$ in a probability space $\Omega$ via the energy function $E(v, h; \Theta)$:
\begin{equation}
   p(x) = p(v, h) = \frac{e^{-E(v,h)}}{Z}.
   \label{eqn:probbm}
\end{equation}
The normalizing factor $Z$, the so called \emph{partition function}, is defined as $Z = \int_{\Omega} e^{-E(x)} dx$. In case of a discrete probability distribution, this can be written as $Z = \sum_{v,h}e^{-E(v,h)}$ where the sum goes over all possible combinations of $v$ and $h$.
The term in the denominator $p^*(v,h) = e^{-E(v,h)}$ is called \emph{unnormalized probability}.
The probability space is divided into dimensions of observed variables (subsumed in vector $v$) and hidden/latent variables (in $h$), corresponding to visible and hidden nodes in the graphical representation, see figure \ref{fig:bmsoverview}.

The so called \emph{free energy}, a notation also inspired by physics, is defined as
\[
   F(v) = - \log \sum_h e^{-E(v, h)}.
\]
With this definition, we can rewrite the formula of the partition function as $Z = \sum_v e^{-F(v)}$.
That is useful because the formula for the free energy of restricted Boltzmann machines can be simplified by using the layerwise structure.
Thus the complexity of calculating the free energy becomes linear in the number of hidden nodes. (This can be seen in the formulas for the free energy in the different types of models that are described below.)
If the partition function $Z$ is given, the log-likelihood
\begin{equation}
   \log p(v) = - F(v) - \log Z
\label{eqn:pRBMfreeenergy}
\end{equation}

can therefore be calculated efficiently using the free energy. The free energy is also used for calculating unnormalized probabilities $p^*(v) = e^{-F(v)}$, which is, e.g., used in the annealed importance sampling algorithm (see \ref{methodAIS}) for estimating the partition function via a stochastic algorithm.

With these basic properties defined, we can go further to take a look at the details of specific types of Boltzmann machines.
Here we only consider the special cases of restricted Boltzmann machines and (multimodal) deep Boltzmann machines.
These models have restrictions on their parameterization compared to a general Boltzmann machine.
The restrictions correspond to a layered design of their graphs.
For an intuitive overview of the architectures of the different types that are considered here, see figure \ref{fig:bmsoverview}.

\begin{figure}[h]

   \centering
   \includegraphics[scale=3.]{images/BMsOverview.eps}
   \caption{Graph view on different types of Boltzmann machines. Visible units (i. e. input units) are depicted as nodes with doubled circle lines. Hidden units are simple circles.
 {\bf A}: General Boltzmann machine, with all nodes connected to each other. {\bf B}: Restricted Boltzmann machine (RBM). Each of the lines corresponds to a weight, i.e. an entry in the weight matrix $W$, like in formulas (\ref{eqn:energyformularbm}) and (\ref{eqn:energyformulagbrbm}).  
{\bf C}: Deep Boltzmann machine. From a graph perspective, this is simply a stack of RBMs.
{\bf D} and {\bf E}: Multimodal/partitioned deep Boltzmann machines. In a multimodal DBM, the different partitions of the visible nodes may also have different distributions.}
   \label{fig:bmsoverview}
 \end{figure}


\subsubsection{Basic properties of restricted Boltzmann machines}\label{rbmtypes}

\emph{Restricted Boltzmann machines} \citep{smolensky1986foundations}, or RBMs,  consist of two layers, a visible layer $v$, which receives the input data, and a hidden layer $h$, which encodes latent features of the data. For a graphical depiction, see figure \ref{fig:bmsoverview}B.
The nodes inside the same layer are not connected to each other, and therefore, the network has the form of a bipartite graph.
Although other distributions are also possible for the hidden nodes \citep{hinton_practical_2012}, we only consider RBMs with Bernoulli distributed hidden nodes, and therefore only consider data representations via discrete features.
The types of RBMs presented in the following may, however, differ in the distribution of their visible nodes, which makes them suitable for modeling different types of input data.

\paragraph{Bernoulli distributed visible nodes}
The most basic model in this class of models are restricted Boltzmann machines with Bernoulli distributed nodes, most of the time simply called restricted Boltzmann machines. Their energy function is of the form
\begin{equation}
   E(v,h) = - a^T v - b^T h - v^T W h. \label{eqn:energyformularbm}
\end{equation}

The parameters of the model are $\Theta = (W, a, b)$ with \emph{weight matrix} $W$, vector $a$ as the \emph{visible bias}, and $b$ as the \emph{hidden bias}.
The weights correspond to the connections between the visible and hidden units in a weighted graph, as shown in figure \ref{fig:bmsoverview}.
The bias variables are usually not depicted in graph views of neural networks, but they are equivalent to adding additional nodes to the model that always have the value one. The visible bias corresponds to the weights on the connections to an additional node that is connected to all nodes of the visible layer, and the hidden bias corresponds to the weights on the connections to an additional node that is connected to all nodes of the hidden layer. The bias variables serve to set a basic level of activation of the nodes, which is then modified by the input from the connected units.
To see the mapping from the parameters in the formula to the network view, see figure \ref{fig:rbmweights}.

\begin{figure}[h]
   \centering
   \includegraphics[scale=0.5]{images/rbmweights.pdf}
   \caption{The nodes and parameters of a restricted Boltzmann machine in the graph view. The second visible node and the first node in the hidden layer are connected with weight $w_{21}$ from the weight matrix $W$. The base-level activity of the nodes $v_2$ and $h_1$ is determined by the biases $a_2$ and $b_1$, respectively.}
   \label{fig:rbmweights}
 \end{figure}


Employing the sigmoid function $\sigm(x) = \frac{1}{1+ e^{-x}}$, the resulting conditional distributions can be written as 
\begin{equation}
p(v_i | h) = \sigm ((a + W h)_i)
 \quad \text{and}\quad 
p(h_i | v) = \sigm ((b + W^T v)_i).
\label{eqn:condprobrbm}
\end{equation}

The free energy is
\begin{equation}
F(v) = - a^T v - \sum_{j=1}^{n_H} \log \left (1 + e^{(W^T v + b)_j}\right).
\label{eqn:freenergy_rbm}
\end{equation}

The trick for deriving the formula of the free energy is to ``analytically sum out" the hidden units \citep{sala2012anefficient}.
The master thesis of \cite{krizhevsky2009tinyimagesthesis} is a very good resource for looking up the complete derivations of these formulas. This holds not only for the theory of RBMs with Bernoulli distributed nodes but also in particular for RBMs with Gaussian distributed visible nodes, which are presented next.

\paragraph{Gaussian distributed visible nodes}\label{gaussianrbm}
One approach for modeling continuous values for $v$ are restricted Boltzmann machines with Gaussian distribution of the visible variables and Bernoulli distributed hidden variables. The energy of the model is defined as
\begin{equation}
   E(v,h) = \sum_{i=1}^{n_V}\frac{(v_i - a_i)^2}{2\sigma_i^2} - b^T h - \sum_{i=1}^{n_V} \frac{(Wh)_i}{\sigma_i}
   \label{eqn:energyformulagbrbm}
\end{equation}

The number of visible and hidden nodes is denoted as $n_V$ and $n_H$, respectively. The parameters of this model are $\Theta = (W, a, b, \sigma)$ with weight matrix $W$, visible bias $a$ hidden bias $b$ and standard deviation $\sigma$. The role of $\sigma$ as standard deviation becomes clearer by looking at the conditional distributions of the $v_i$ given $h$, which are distributed according to the normal distribution $\mathcal{N}(a_i + \sigma_i(Wh)_i, \sigma_i^2)$. For the hidden nodes, $p(h_j | v) = \sigm \left (b_j + \sum_{i=1}^{n_V} w_{ij} \frac{v_i}{\sigma_i} \right )$.
The free energy is
\begin{equation}
   F(v) = \sum_{i=1}^{n_V}\frac{(v_i - a_i)^2}{2\sigma_i^2} + \sum_{j=1}^{n_H} \log \left (1 + e^{b_j + \sum_{i=1}^{n_V} \frac{v_i}{\sigma_i} w_{ij}} \right)
\label{eqn:freenergy_gbrbm}
\end{equation}
\citep{krizhevsky2009tinyimages}.

Cho et al. \citep{cho2011improved} proposed a different parameterization for the energy function:
\begin{equation}
   E(v,h) = \sum_{i=1}^{n_V}\frac{(v_i - a_i)^2}{2\sigma_i^2} - b^T h - \sum_{i=1}^{n_V} \frac{(Wh)_i}{\sigma_i^2}
   \label{eqn:energyformulagbrbm2}
\end{equation}
In this model the distribution of the visible nodes is the multimodal normal distribution $\mathcal{N}(a + Wh, \sigma^2)$.
The conditional distribution of the hidden nodes is $p(h_j | v) = \sigm \left (b_j + \sum_{i=1}^{n_V} w_{ij} \frac{v_i}{\sigma_i^2} \right )$, and the free energy is
\begin{equation*}
   F(v) = \sum_{i=1}^{n_V}\frac{(v_i - a_i)^2}{2\sigma_i^2} + \sum_{j=1}^{n_H} \log \left (1 + e^{b_j + \sum_{i=1}^{n_V} \frac{v_i}{\sigma_i^2} w_{ij}} \right).
\end{equation*}

%TODO cite: 
%\citep{melchior_gbrbm}, \citep{cho_gaussiandbm}
% TODO mention alternative: use continuous as Gaussian hard., verweis auf entsprechenden results teil


\subsubsection{Gibbs sampling in restricted Boltzmann machines}\label{gibbssamplingrbm}
In the previous sections concerning RBMs with different input distributions, one could see that it was possible to specify a formula for the conditional probability that has the same  algorithmic complexity of the matrix multiplication in all cases.
However, the formula for the simple probability $p(v)$ of a sample $v$ requires calculating the partition function $Z$ (see formula (\ref{eqn:probbm})), and calculating $Z$ is not feasible for most practical scenarios, as the number of summands in $Z$ grows exponentially with the number of nodes in the model (see also later in \ref{methodExactloglik}).
This means that using RBMs as generative models, i.e. drawing samples according to the distribution captured in its parameters, is not as straightforward as calculating $p(v)$ and sampling according to the distribution.

But because of the layered structure of RBMs and the closed form of the conditional distributions $p(v \mid h)$ and $p(h \mid v)$, which is fast to evaluate, it becomes possible to efficiently apply a Markov chain Monte Carlo technique called \emph{Gibbs sampling} \citep{gibbssamplingorig} for approximating the distribution of the RBM.
The Gibbs sampling  algorithm for RBMs can be formulated as follows:

\begin{enumerate}
\item Start with an arbitrary $v_1$.
\item Draw $h_1$ according to $p(h \mid v_1)$.
\item Draw $v_2$ according to $p(v \mid h_1)$.
\item Repeat steps 2 and 3 until convergence.
\item Result: After $n$ steps, $v_n$ and $h_n$ have been drawn according to the joint distribution $p(v,h)$ of the model.
\end{enumerate}

This works because the iteration forms a Markov chain that has the distribution $p(v,h)$ of the model as its equilibrium distribution.

\paragraph{Conditional sampling}\label{condsamplingrbm}
The Gibbs sampling algorithm can also be easily modified for sampling conditionally on the activations of input nodes by clamping the activations of the nodes that are to be conditioned on and running the Gibbs sampling algorithm in the rest of the network.
Put more formally, to draw from the probability $p(v, h \mid \tilde{v}_C)$ that is conditioned on the activations $\tilde{v}_C$ of a set $C$ of visible nodes, the Gibbs sampling algorithm above can be run with $v_C$ set to $\tilde{v}_C$ in step 1 and after each sampling step 3.
This, of course, works analogously for hidden nodes as conditions.


\subsubsection{Training of restricted Boltzmann machines}\label{rbmtraining}
Goal of the training procedure of restricted Boltzmann machines is to maximize the likelihood $\prod_{k=1}^n p(\widetilde{v}^{(k)})$ for a given data set $(\widetilde{v}^{(1)}, \dots, \widetilde{v}^{(n)})$.
Maximizing the likelihood is equal to maximizing the log-likelihood
\[
\sum_{k=1}^{n}  \log p(\widetilde{v}^{(i)}) = \sum_{k=1}^{n} \left( \log \sum_h e^{-E(\widetilde{v}^{(k)}, h)} - \log \sum_v \sum_h e^{-E(v, h)} \right).
\]


\paragraph{Deriving the gradient of the log-likelihood}

For finding an optimum for $\sum_{k=1}^{n}  \log p_\Theta(\widetilde{v}^{(k)})$, the gradient $\nabla_\Theta \sum_{k=1}^{n}  \log p_\Theta(\widetilde{v}^{(k)})$ needs to be determined.
The gradient of the log-likelihood for a single observation
$\widetilde{v} $ with respect to the parameter set $\Theta$ is
\begin{align}
\nabla_{\!\Theta}  \log p(\widetilde{v}) &= \nabla_{\!\Theta}   \log \sum_h e^{-E(\widetilde{v}, h)} - \log \sum_v \sum_h e^{-E(v, h)} \nonumber \\
 &= \frac{\nabla_{\!\Theta} \sum_h e^{-E(\widetilde{v}, h)}}{\sum_h e ^{-E(\widetilde{v}, h)}} - \frac{\nabla_{\!\Theta} \sum_v \sum_h e^{-E(v,h)}}{\sum_v \sum_h e^{-E(v,h)}} \label{eqn:derivedlog}\\
&=  \frac{\sum_h e^{-E(\widetilde{v}, h)} \nabla_{\!\Theta} (-E(\widetilde{v}, h))} {\sum_h e^{-E(\widetilde{v}, h)}} - \frac{\sum_v \sum_h e^{-E(v, h)} \nabla_{\!\Theta} (-E(v, h))}{\sum_v \sum_h e^{-E(v, h)}} \label{eqn:derivedexp}\\
&=  \EX_{P_\text{data}} \nabla_{\!\Theta} (- E(\widetilde{v},h)) - \EX_{P_\text{model}} \nabla_{\!\Theta} (-E(v,h)) \label{eqn:nablaresult}
\end{align}
In (\ref{eqn:derivedlog}) and (\ref{eqn:derivedexp}), the chain rule is used, together with the derivative of the logarithm and the exponential function, respectively. The resulting terms can be rewritten in equation (\ref{eqn:nablaresult}) as the expectation of the gradient given the distribution of the data and the expectation of the gradient given the distribution of the model.

As can be seen in equation (\ref{eqn:derivedexp}), the calculation of the gradient involves sums over lots of possible combinations of activations of nodes.
The first term can be calculated in the types of RBMs that have been introducted here.
For this, one can use the expected value of the conditional probability $p(h\mid\widetilde{v})$ and plug it in:
\begin{equation}
\EX_{P_\text{data}} \nabla_{\!\Theta} E(\widetilde{v},h) =  \nabla_{\!\Theta} E(\widetilde{v}, \EX(p(h \mid \widetilde{v}))
\label{eqn:plugexpectedh}
\end{equation}
In the second term of equation (\ref{eqn:nablaresult}), the sum goes over all nodes in the network.
Calculating this term is technically not feasible in normal cases, as the number of summands grows exponentially with the number of nodes.
This means that it is not possible to simply equate the gradient of the log-likelihood with zero and solve the equation to get an optimum. Also finding an optimum with gradient descent \citep{gradientdescent}, walking with small steps in the direction of the gradient to find an optimum, is not possible directly, because this would as well require the calculation of the whole gradient in each iteration step.
Via Gibbs sampling (see section \ref{gibbssamplingrbm}), however, the term can be estimated. 
With this, it becomes possible to use the estimated gradients in gradient descent, or here rather ``gradient ascent" as we would like to find a (local) maximum.

\paragraph{Batch gradient optimization and the role of mini-batches}

For the practical implementation, another stochastic approximation of the gradient for the full data set $\sum_{k=1}^n \log p(\widetilde{v}^{(k)})$ is used.
Usually a form of {\em batch gradient optimization} \citep{bottou_optimization_2018} is performed.
For this, the samples are split into batches $B_l$, usually of approximately equal sizes $b_l$.
In each optimization step $t$, the parameters are updated as follows:
\[
\Theta^{(t+1)} = \Theta^{(t)} + \frac{\epsilon}{b_l} \sum_{\widetilde{v} \in B_l} \nabla_{\!\Theta^{(t)}} \log p(\widetilde{v})
\]

The step size is determined by the {\em learning rate} $\epsilon$.
Each iteration $t$ calculates the gradient of a different batch $B_l$ until all samples have been used.
The mini-batches are usually reused multiple times for the learning.
The number of steps after all samples/mini-batches are used for updating the parameters, is called a {\em (training) epoch}.
(In the formula above one training epoch is over if $t = \sum_l b_l$.)
Training usually consists of many epochs and the step size is kept small, e.g. a hundred epochs and a learning rate of 0.001 could be a viable combination.
The number of epochs and the learning rate are the most important hyperparameters for the training.

%TODO optional: hier könnte man mehr schreiben über zusammenspiel von learning rate und epochs, außerdem steps auf oberfläche beschreiben und learning rate decay

Mini-batches were at first introduced for performance reasons.
It is not necessary to have the exact gradient for walking only small steps into the direction of it.
So it is sufficient to calculate the gradient for small subsets of the data, which is noisy, but will be correct on average.
Using mini-batches can also be advantageous because of another reason: The variance of the steps becomes higher with a with a smaller batch size. This leads to exploring more of the surface of the likelihood function in the high-dimensional space and therefore can lead to finding better local optima in some scenarios \citep{bengio2012practical}.

\paragraph{Contrastive divergence}

For performing batch gradient optimization, the Gibbs sampling procedure is also modified for RBM training to speed up the learning process.
In other applications, thousands of iterations are used for Gibbs sampling  \citep{gibbssamplingorig}. For training RBMs, a shortcut is used, which is called {\em contrastive divergence} (CD) \citep{cdorig, perpinan_contrastive_2005}.
In CD, the number of Gibbs sampling steps is reduced drastically. In most cases, even only one step is used, which is denoted with $\text{CD}_1$ \citep{hinton_practical_2012}.
Instead of using a random starting point, CD uses the original sample $\widetilde{v}$, for which the gradient shall be computed, as starting point $v_1$ of the sampling procedure. This should ensure that the starting point is already close to the desired distribution.

Another modification of the Gibbs sampling procedure used for training of RBMs is {\em persistent contrastive divergence} (PCD).
Similar to CD, this procedure also uses only very few steps. Here, the state of the Gibbs sampling chain for calculating the last update is reused and used as starting point. \cite{hinton_practical_2012} recommends PCD over $\text{CD}_1$ or even $\text{CD}_{10}$.

\paragraph{Gradients for different types of restricted Boltzmann machines}\label{rbmgradients}

In an RBM with Bernoulli distributed nodes, equation (\ref{eqn:nablaresult}) leads to the following formulas for the gradients for the weights and biases:
\begin{align*}
\frac{\partial}{\partial W}  \log p(\widetilde{v}) &= \EX_{P_\text{data}}  \widetilde{v}^T h - \EX_{P_\text{model}} v^T h \\
\frac{\partial}{\partial a}  \log p(\widetilde{v}) &=  \EX_{P_\text{data}} 
\widetilde{v} - \EX_{P_\text{model}}  v = \widetilde{v} - \EX_{P_\text{model}} v\\
\frac{\partial}{\partial b}  \log p(\widetilde{v}) &=  \EX_{P_\text{data}} h - \EX_{P_\text{model}} h 
\end{align*}

In an RBM with Gaussian visible nodes \citep{krizhevsky2009tinyimagesthesis}, the  gradients are:
\begin{align*}
\frac{\partial}{\partial W} \log p(\widetilde{v}) &= \frac{1}{\sigma_i} \left ( \EX_{P_\text{data}} \widetilde{v}^T h - \EX_{P_\text{model}} v^T h \right) \\
\frac{\partial}{\partial a} \log p(\widetilde{v}) &= \frac{1}{\sigma_i^2} \left(\widetilde{v} - \EX_{P_\text{model}} v \right)\\
\frac{\partial}{\partial b} \log p(\widetilde{v}) &= \EX_{P_\text{data}} h - \EX_{P_\text{model}} h \\
\frac{\partial}{\partial \sigma_{i}} \log p(\widetilde{v}) &= \EX_{P_\text{data}} \left( \frac{(\widetilde{v}_i - a_i)^2}{\sigma_i^3} - \sum_{j=1}^{n_H} h_j \frac{w_{ij} \widetilde{v}_i}{\sigma_i^2}\right) \\ & \quad \quad - \EX_{P_\text{model}} \left(\frac{(v_i - a_i)^2}{\sigma_i^3} -\sum_{j=1}^{n_H} h_j \frac{w_{ij} v_i}{\sigma_i^2} \right)
\end{align*}

In the alternative formulation of \cite{cho2011improved}, the gradients are:
\begin{align*}
\frac{\partial}{\partial W} \log p(\widetilde{v}) &= \frac{1}{\sigma_i^2} \left ( \EX_{P_\text{data}} \widetilde{v}^T h - \EX_{P_\text{model}} v^T h \right) \\
\frac{\partial}{\partial a} \log p(\widetilde{v}) &= \frac{1}{\sigma_i^2} \left(\widetilde{v} - \EX_{P_\text{model}} v \right)\\
\frac{\partial}{\partial b} \log p(\widetilde{v}) &= \EX_{P_\text{data}} h - \EX_{P_\text{model}} h \\
\frac{\partial}{\partial \sigma_{i}} \log p(\widetilde{v}) &=  \frac{1}{\sigma_i^3} \Bigg( \EX_{P_\text{data}} \left( (\widetilde{v}_i - a_i)^2 - 2\sum_{j=1}^{n_H} h_j w_{ij} \widetilde{v}_i  \right) \\ & \quad \quad - \EX_{P_\text{model}} \left((v_i - a_i)^2 -2\sum_{j=1}^{n_H} h_j w_{ij} v_i \right) \Bigg)
\end{align*}

In the RBM with the dummy variables for encoding categorical variables, the gradients are the same as in the one with Bernoulli distributed variables.

Using the gradients specified above, it is possible to run the gradient optimization for training the different types of RBMs.

\subsubsection{Deep belief networks}\label{dbns}

The next step in the evolution deep Boltzmann machines are deep belief networks (DBNs) \citep{hinton_reducing_2006}, which have been invented to make better use of RBMs for dimensionality reduction.
Detecting higher level features in data is hard using shallow networks such as RBMs.
The idea of DBNs is to stack RBMs on top of each other (see figure \ref{fig:dbn}) to be able to model features of increasing complexity with an increased depth of the network.

\begin{figure}[h]
   \centering
   \includegraphics[scale=0.7]{images/dbn.pdf}
   \caption{Training a deep belief network (DBN) and using it as generative model.
   {\bf A}: The DBN is trained as a stack of RBMs. {\bf B}: In a DBN as generative model, only the top RBM is used for Gibbs sampling, then the activation is passed in a deterministic way downwards to the visible layer.}
   \label{fig:dbn}
 \end{figure}
 
For training a DBN, the first RBM is trained with the original data data vectors $\tilde{v}^{(i)}$ with $i = 1, \dots, n$. 
For training the higher layers, a deterministic activation $f_k(v) :=  \EX p_k(h|v)$, defined via the conditional probability in the $k$-th RBM model, is passed through the lower layers after they have been trained:
The second RBM is trained with the data set of all $f_1(\tilde{v}^{(i)})$, the third RBM can then be trained with the data set $f_2(f_1(\tilde{v}^{(i)}))$, and so on.

As shown in figure (\ref{fig:dbn}B), only the last RBM in the network can be used for sampling, which limits the generative capabilities of a DBN.
The ability to generate data from complex distributions is furthermore reduced if the DBN is used for dimension reduction because in this case the top RBM is also the smallest network.


\subsubsection{Deep Boltzmann machines}

If the network architecture of a DBN is treated as a Boltzmann machine, we get a \emph{deep Boltzmann machine}, where the complete network can be utilized for generating samples.
A deep Boltzmann machine, as defined by \cite{salakhutdinov2009DBMs}, with $n_L$ number of hidden layers has parameters 
\[
\Theta = \left (W^{(1)}, \dots, W^{(n_L)}, a, b^{(1)}, \dots, b^{(n_L)} \right).
\]
The energy is defined as 
\[
E(v, h^{(1)}, \dots, h^{(n_L)}) = - a^T v - v^T W^{(1)} h^{(1)} - \sum_{k=1}^{n_L} b^{(k)} h^{(k)} -  \sum_{k=2}^{n_L} h^{(k-1)}W^{(k)}h^{(k)}.
\]
For the connection between the formula and the resulting network architecture see figure  \ref{fig:dbmweights}.
\begin{figure}[h]
   \centering
   \includegraphics[scale=0.6]{images/dbmweights.pdf}
   \caption{The nodes and parameters of a deep Boltzmann machine in the graph view}
   \label{fig:dbmweights}
 \end{figure}
 
A deep Boltzmann machine has Bernoulli distributed nodes.
Due to the layer-wise structure, the conditional probability of the visible nodes $p(v \mid h) = p(v \mid h^{(1)})$ depends only on the first hidden layer and can be calculated like in an RBM with Bernoulli distributed nodes.
The conditional probability of the final hidden layer $p \left( h^{n_L} \mid v, h^{(1)}, \dots, h^{(n_L -1)} \right) = p \left( h^{n_L} \mid h^{(n_L -1)} \right)$ can similarly be calculated by knowing only on $h^{(n_L-1)}$.
For the intermediate layers, the conditional probability can be calculated if the neighboring layers are given as
\begin{equation}
p\left(h^{(1)} \mid v, h^{(3)}\right) = \sigm \bigg( b^{(1)} + (W^{(1)})^T v + W^{(3)} h^{(3)} \bigg)
\label{dbmcondprobfirsthidden}
\end{equation}
for the first hidden layer and 
\begin{equation}
p\left(h^{(k)} \mid h^{(k-1)}, h^{(k+1)} \right) = \sigm \bigg( b^{(k)} + (W^{(k-1)})^T h^{(k-1)} + W^{(k+1)} h^{(k+1)} \bigg)
\label{dbmcondprobintermediate}
\end{equation}
for all other intermediate hidden layers.

These conditional probabilities can be used to perform Gibbs sampling in deep Boltzmann machines (see figure \ref{fig:dbmsampling}) similar to the Gibbs sampling algorithm in restricted Boltzmann machines (as described before in \ref{condsamplingrbm}).

\begin{figure}[h]
   \centering
   \includegraphics[scale=1.0]{images/dbmsampling.pdf}
   \caption{Gibbs sampling step in a deep Boltzmann machine. The visible layer is sampled using only the activation of the first hidden layer from the previous Gibbs sampling step, and the last hidden layer is sampled using only the activation of the penultimate hidden layer. The intermediate layers are sampled by using the activations of the two neighboring layers from the previous Gibbs sampling step. }
   \label{fig:dbmsampling}
 \end{figure}


\subsubsection{Training of deep Boltzmann machines}\label{dbmtraining}


In context of DBMs, the DBN learning algorithm (see \ref{dbns}) is called greedy layerwise pre-training. 
This pre-training helps the algorithm for training a general Boltzmann machine to find a better optimum.
An intermediate layer in a DBM receives input from its neighboring layers (see formula (\ref{dbmcondprobintermediate}) and figure \ref{fig:dbmsampling}), but during pre-training it only gets input from the layer below. To account for this, the DBN training is modified for training a DBM and the input from the visible layer ($ = (W^{(1)})^T v$) and from the last hidden layer (= $W^{(n_L-1)} h^{(n_L)}$) is doubled.
\citep{salakhutdinov2009DBMs}.


Using the pre-trained DBM as starting point, the fine-tuning of the weights can then be performed by the algorithm for training a general Boltzmann machine
\citep{salakhutdinov2009DBMs, salakhutdinov2015generativemodels}.
This algorithm is also based on gradient descent.
In RBMs it is possible to calculate $p(h | v)$ because all the hidden nodes are only depending on the visible nodes.
This allowed to calculate $\EX_{P_{\text{data}}} \nabla_{\!\Theta} E(\tilde{v},h)$ from $\EX p(h | \tilde{v})$ (see formula (\ref{eqn:plugexpectedh})).
In DBMs this is not possible any more as the hidden layers are connected with each other and therefore no simple formula for $p(h | v)$ can be derived.
This means that calculating the derivative of the log-likelihood in deep Boltzmann machines needs an additional technique.
\cite{salakhutdinov2009DBMs} propose a variational approach for this.
The true distribution $p(h|v)$  is replaced with an approximation $q(h|v)$.
Instead of optimizing the log-likelihood, a lower bound is optimized, for which the gradient can be computed.
This \emph{variational lower bound} or \emph{evidence lower bound} (ELBO) \citep{blei_variational_2017} for the likelihood is
\[
\log p(v) \geq \sum_h q(h|v) \log p(v,h) + \mathcal{H}(q)
\]
$\mathcal{H}(q)$ denotes the entropy of the distribution $q$.
Here a ``mean-field approximation" $q$ is used, where all hidden nodes are assumed to be independent.
The distribution is then $q(h) = \prod_{k=1}^{n_L} \prod_j^{n_{H,k}} q(h_i^{(k)})$ with $q(h_i^{(k)} = 1) = \mu_i^{(k)}$ for some fixed values $\mu_i^{(k)}$ for each  node $i$ in a hidden layer $k$.
($n_L$ denotes the number of hidden layers and $n_{H,k}$ the number of nodes in hidden layer $k$.)
This leads to the lower bound of the log-likelihood, which will be optimized in the fine-tuning algorithm: %TODO ableiten? steht in sala learning deep generative models equation 36
\[
   \log p(v) \geq - E(v, \mu) - \log Z + \mathcal{H}(q) =: \ELBO(v, \mu)
\]
$E$ is the energy in the DBM and $\mathcal{H}(q) = \sum_j \left( \mu_j \log \mu_j + (1- \mu_j) \log ( 1- \mu_j) \right)$ is the entropy of $q$ \citep{sala2012anefficient, salakhutdinov2015generativemodels}.

To calculate $\mu$ for a given data vector $\tilde{v}$, the following equations for a fix-point iteration can be used, which are analogous to formulas (\ref{dbmcondprobfirsthidden}) and (\ref{dbmcondprobintermediate}).
\begin{align*}
\mu^{(1)}(t+1)&= \sigm \bigg( b^{(1)} + (W^{(1)})^T \tilde{v} + W^{(2)} \mu ^{(2)}(t) \bigg) \\
\mu^{(k)}(t+1) &= \sigm \bigg( b^{(k)} + (W^{(k)})^T \mu^{(k-1)}(t) + W^{(k+1)} \mu^{(k+1)}(t) \bigg) \\ 
&\quad\quad (\text{for} \; k=2,\dots, n_L-1) \\
\mu^{(n_L)}(t+1)&= \sigm \bigg( b^{(n_L)} + (W^{(n_L)})^T \mu^{(n_L-1)}(t) \bigg)
\end{align*}
For  $t \rightarrow \infty$, the series $\mu(t)$ converges to the value of $\mu$ that can be used for calculating the gradient for the lower bound.
As a starting value for $\mu$, activation in the network is induced by the input using a single forward pass, treating the DBM as a DBN (see \ref{dbns}).

Analogously to equation (\ref{eqn:nablaresult}) for calculating the gradient of the log-likelihood in RBMs, the gradient of the variational lower bound of the log-likelihood can be calculated as 
\begin{align*}
\nabla_{\Theta} \ELBO(v, \mu) &= \nabla_{\Theta} ( - E(\tilde{v}, \mu) - \log Z ) \\
 &= \nabla_{\Theta} (- E(\tilde{v}, \mu)) - \EX_{P_\text{model}} \nabla_{\Theta} (-E(v, h)).
\end{align*}
This results in the following gradients for the different types of model parameters:
\begin{align*}
\frac{\partial}{\partial W^{(1)}} \ELBO(\tilde{v}, \mu) &= \widetilde{v}^T \mu - \EX_{P_\text{model}} v^T h^{(1)} \\
\frac{\partial}{\partial W^{(k)}} \ELBO(\tilde{v}, \mu) &= (\mu^{(k-1)})^T \mu^{(k)}  - \EX_{P_\text{model}} (h^{(k-1)})^T h^{(k)} &\quad(k = 1, \dots, n_L)\\
\frac{\partial}{\partial a}  \ELBO(\tilde{v}, \mu) &=  \widetilde{v} - \EX_{P_\text{model}}  v \\
\frac{\partial}{\partial b^{(k)}}  \ELBO(\tilde{v}, \mu) &=  \mu^{(k)} - \EX_{P_\text{model}} h^{(k)}  &\quad ( k = 1, \dots, n_L)
\end{align*}

The expected values under the distribution of the model ($\EX_{P_\text{model}}$) are estimated like in the RBM by running a Gibbs sampler (see figure \ref{fig:dbmsampling}).
This Gibbs sampler is run with a number of parallel persistent chains, which are called ``fantasy particles" \citep{salakhutdinov2009DBMs}. The results are averaged over the number of fantasy particles to get better estimations for the expected values in the distribution of the model.
For the convergence of the gradient optimization, it is necessary to decrease the learning rate $\epsilon_t$ over time such that the series $\sum_{k=1}^\infty \epsilon_t^2$ converges, e.g. $\epsilon_t = \frac{c}{d+t}$ with constants $c, d > 0$ \citep{sala2012anefficient}.

Since the training procedure is very complex and depends on many hyperparameters, it is necessary to test whether the training works well by examining the training objective.
Although the likelihood is not the best criterion for all kinds of applications \citep{theis_note_2015}, it is essential to have a way to get a value for the likelihood as primary optimization criterion.
Being able to inspect the learning process is important for finding the best choice of hyperparameters and also for ensuring the quality of the software implementation of the learning algorithm.
This leads us to the next section, where we want to take a closer look at methods for calculating and estimating the likelihood, and the lower bound of the likelihood, in case of DBMs.


\subsubsection{Evaluating restricted and deep Boltzmann machines}
A special challenge for unsupervised learning on non-image data in general is the lack of performance indicators.
In supervised training, the classification accuracy is the natural evaluation criterion, which is also easy to implement.
In unsupervised training with a well investigated class of data such as images, there is already much experience available for choosing the model architecture and the hyperparameters. If models are to be trained on very diverse data, the problem of finding good hyperparameters is exacerbated as parameter tuning can pose a different challenge for each data set.

Thus the need for having an objective evaluation criterion as a basis for choosing the hyperparameters and monitoring the training becomes very important.
In case of images or natural language, the generative abilities of a model can be tested by simply looking at the generated images or sentences to see whether these are proper samples. In the case of data from a patient record or genetic data, this approach is not feasible.

If there are no other evaluation criteria, one indicator for successful learning in Boltzmann machines remains the model likelihood, which is an inherent property of the model and is therefore applicable in all cases of data. The difficulty for calculating the likelihood is its dependency on the partition function (see formula (\ref{eqn:probbm})).
In most cases, the likelihood cannot be calculated exactly but it can only be estimated by stochastic algorithms like annealed importance sampling (AIS). 
% So we also want to detail the extension of AIS on multimodal deep Boltzmann machines in this article.

\paragraph{Exact calculation of the partition function}
\label{methodExactloglik}
As mentioned in section \ref{rbmtraining}, the exact calculation of partition functions is only computationally feasible for very small models as its complexity grows exponentially. Exploiting the layerwise structure allows a faster exact calculation of $Z$ such that the computation time does not grow exponentially with the number of all nodes but only grows exponentially with the number of elements in a subset of the nodes. It is possible to utilize the formula for the free energy in restricted Boltzmann machines (see formulas (\ref{eqn:freenergy_rbm}) and (\ref{eqn:freenergy_gbrbm})), where the hidden layer is summed out analytically.
With this it is possible to reduce the number of summands. 
The complexity for calculating the partition function for all the different types of  models described here is then still $\mathcal{O}(2^n)$, but with an $n$ smaller than the number of nodes:

By using the formulas for the free energy and the symmetry of restricted Boltzmann with binary nodes, $n = \min(n_V, n_H)$ with $n_V$.
In RBMs with one of layer Gaussian nodes and one layer of binary nodes, $n$ is the number of binary nodes, since the contribution of the Gaussian nodes can be integrated analytically.
In case of a deep Boltzmann machine, it is possible to sum out each second layer, similar to the calculation of the free energy in restricted Boltzmann machines. So for DBMs, $n$ can be reduced to the number of nodes in each second layer, see figure \ref{aissummingout}.

\begin{figure}[h!]
\centering
\includegraphics[scale=2.5]{images/AISsummingout.pdf}
\caption{Possibilities for summing out layers for calculating the likelihood. For calculating the likelihood in a more efficient way, the layers in the gray boxes can be summed out analytically. {\bf A:} Summing out the odd layers in this DBM leaves $2^{(4 + 4)} = 256$ summands that still have to be summed in order to calculate the likelihood. {\bf B:} Summing out the even layers in this DBM leaves in $2^{(4 + 3)} = 128$ summands. {\bf C:} Summing out the even layers in  this multimodal DBM leaves $2^{(6 + 3)} = 512$ summands.}
\label{aissummingout}
\end{figure}

\paragraph{Estimating partition functions with annealed importance sampling (AIS)}\label{methodAIS}

For annealed importance sampling we need a sequence of intermediate distributions
$p_0, \dots p_K$ with
$p_0 = p_A$ and $p_K = p_B$. The ratio $\frac{Z_B}{Z_A}$ is then estimated by the mean of a number of so called importance weights that are determined as
\[
   \prod_{k=1}^K \frac{p^*_k(x_k)}{p^*_{k-1}(x_{k})}.
\]
The $x_k$ are produced by iteratively performing Gibbs sampling. Starting with $x_0$ sampled from $p_0$, one obtains $x_k$ by sampling a Gibbs chain that is initialized with $x_{k-1}$ in an intermediate model with distribution $p_k$. An appropriate choice for the intermediate models are Boltzmann machines with the energy functions $E_k$ chosen such that
\[
   E_k(x) = (1 - \beta_k) E_A(x) + \beta_k E_B(x)
\]
and therefore
\[
   p_k^*(x) = p_A^*(x)^{1-\beta_k} p_B^*(x)^{\beta_k}.
\]
The factors $\beta_k$ with $0 = \beta_0 < \beta_1 < ... < \beta_K = 1$ are called temperatures \citep{salakhutdinov2008learning}.
The choice of the temperatures, the number of importance weights and also the number of Gibbs sampling steps for the transition are hyperparameters for the AIS algorithm.

With AIS, it is possible to get a direct estimate of $Z$ by annealing from the full model with distribution $p_A$ to a null model with all weights being zero, for $p_B$.
The partition function $Z_B$ of the null model can be calculated directly, and therefore $Z_A$ can be computed from the AIS estimator of $\frac{Z_B}{Z_A}$. This approach is shown in figure \ref{figTwotypesais}A.

It is also possible to compare the likelihood of two models instead of calculating it directly.
For this, only the ratio $\frac{Z_B}{Z_A}$ between two full models needs to be estimated.
There are two practical approaches for constructing intermediate models  for directly annealing from one full model to another full model (see also figure \ref{figTwotypesais}, B and C):
\begin{enumerate}
\item Combining (adding) corresponding weights of two models of the same size to get an intermediate model of the same size.
\item Constructing a larger model by putting the two models next to each other and connecting their nodes, increasing the energy in one part of the combined model while reducing it in the other part \citep{theis2011deepbelief}:
\end{enumerate}
\begin{figure}[h!]
\centering
\includegraphics[scale=3.5]{images/twotypesais.pdf}
\caption{Different approaches for performing AIS. 
The tempered weights of the intermediate distributions are shown in grey. Black lines correspond to the original weights of a model. No lines between nodes are equal to the weights being zero.
{\bf A:} Annealing from a null model, where all weights are zero, to the full model.
{\bf B:} Annealing from one RBM model with three visible nodes and two hidden nodes to another model of the same size.
The weights of the first model are depicted as continuous lines, the weights of the second as dashed lines. Here, model one is gradually morphed into model two. {\bf C:} 
The same two RBMs are compared again by annealing from a model that is equivalent to the first model to a model that is equivalent to a second model via a combined, bigger model. For this approach, both models do not necessarily have to be of the same size.}
\label{figTwotypesais}
\end{figure}
The approach with a combined model (see figure \ref{figTwotypesais}C) is more flexible and allows to estimate the ratios of two arbitrary Boltzmann machines of the same type and with the same number of hidden nodes.
But it requires sampling in a Boltzmann machine that has as many hidden nodes as the two models together.

% TODO verweis, was im package benutzt wird.


In a DBM, it is possible to use only the states of each second layer from samples generated by running a Gibbs chain \citep{salakhutdinov2008learning}. The unnormalized probabilities of these states can be calculated by analytically summing out the other layers.
For this, we can use the fact that the states in one layer are independent from the states of each non-adjacent layer to get the unnormalized probability $p_k^*(\tilde{h})$ for the subset $\tilde{h} = \left(h^{(1)}, h^{(3)}, \dots\right)$ of a generated sample $x = \left( v, h^{(1)}, h^{(2)}, \dots \right)$ in an intermediate model. 
The layers that can be analytically summed out are the same as the ones shown in figure \ref{aissummingout}.
To calculate the ratio $\frac{p_k^*(x_k)}{p_{k-1}^*(x_{k})}$, one can derive the formula for $p^*(\tilde{h})$ in a DBM similarly to the free energy $F(v) = - \log p^*(v)$ in RBMs \citep{sala2012anefficient}. For a DBM with three hidden layers, the formula is
\begin{align}
p^*( \tilde{h} ) =&  e^{(b^{(1)})^T h^{(1)}} \prod_j (1+ \exp((a + W^{(1)} h^{(1)})_j)) \; \cdot \nonumber \\ 
&e^{(b^{(3)})^T h^{(3)}} \prod_{j}  \left( 1 + \exp \left( (b^{(2)} + (W^{(2)})^T h^{(1)} + W^{(3)} h^{(3)})_j \right) \right).
\label{eqn:aisunnormalizedprob}
\end{align}

Here the visible layer $v$ and the second hidden layer $h^{(2)}$ have been summed out analytically, like shown in figure \ref{aissummingout}B.

%Note that the model for Gibbs sampling, used as the Markov transition operator, can differ from the model that is used to evaluate the unnormalized probability $p_k^*(h)$ as long as the probability distribution $p_k(h)$ of the hidden nodes is the same.
%This trick can be used to fit AIS for GBRBMs in the same schema as RBMs with Bernoulli distributed nodes.
It can be noted that the term in the first line of formula \ref{eqn:aisunnormalizedprob} is equal to the unnormalized probability of the hidden nodes in a RBM with Bernoulli distributed nodes.
This procedure can be generalized for AIS on multimodal DBMs. 
If we have the unnormalized probability $p^*(h)$ for each type of RBM that receives the data input, it becomes possible to calculate the unnormalized probability of sampled hidden values in a multimodal DBM in the same way as for a standard DBM with only Bernoulli distributed nodes.
The formula for the unnormalized probability for the respective RBM type (see section \ref{unnormalizedprobsrbm}) can then be used for summing out the visible units in formula \ref{eqn:aisunnormalizedprob} by substituting the term in the first line with the product of the unnormalized probabilities for all RBMs in the visible layer.

The formulas for $p^*(h)$ for each different type of RBM are derived next, also by analytically ``summing out", or in case of Gaussian RBMs, ``integrating out" the visible layer.




\paragraph{Calculating or estimating likelihoods in deep Boltzmann machines} %TODO verbessern
For a restricted Boltzmann machine, the likelihood can be calculated using formula (\ref{eqn:pRBMfreeenergy}) if the partition function is known. This is not so easily possible in a DBM, for which calculating the distribution of the hidden nodes is of exponential complexity.
Estimating the likelihood of DBMs is possible using AIS by constructing a smaller DBM for each sample and estimating its partition function.
The smaller DBM is constructed by removing the visible layer, and incorporating contribution of the sample to the energy of the first RBM - consisting only of visible and first hidden layer - into the bias of the new visible layer which was the first hidden layer of the original model.
The partition function of this smaller model is then the unnormalized probability of the sample in the original model.
In a setting with very large sample size, the cost of estimating the actual likelihood with this procedure is too expensive. But if the sample size is small enough, it is affordable to estimate the likelihood and not fall back on the lower bound.

\paragraph{Alternatives to the likelihood for RBMs}
The AIS algorithm is very complex to implement.
It is also compute-intensive, depending on the required exactness of the estimation.
So often there are other statistics used for checking the training progress.

The free energy cannot be used for comparing different models because it does not include the normalization by $Z$.
It can, however, be used to compare how well the same model fits different data sets.
One application for this is monitoring the overfitting by comparing the training data set and a test data set \citep{hinton_practical_2012}.
\label{reconstructionerror}
Another popular statistics, which behaves similar to the likelihood in RBMs in most cases, is the {\em reconstruction error} \citep{hinton_practical_2012}.
For defining this, one needs at first to define the term {\em reconstruction} in an RBM.
The reconstruction \citep{hinton_practical_2012} for a sample $\widetilde{v}$ is calculated by using the conditional probabilities as deterministic ``activation potential".
With $f_h(h) := \EX_v p(h|v)$ and $f_v(h) := \EX_h p(v|h)$, the reconstruction $r(v)$ can be defined as $r(\widetilde{v}) := f_v(f_h(\widetilde{v}))$. The reconstruction error is then the distance between the sample $\widetilde{v}$ and its reconstruction $r(\widetilde{v})$, e.g. measured as absolute distance or quadratic distance.

Calculating the reconstruction error  is a very fast and simple technique for monitoring the training progress for RBMs, DBNs and for greedy layer-wise pre-training of DBMs.
Although the reconstruction error can serve as a rough proxy statistic for the likelihood, it does not replace the likelihood entirely, as it is not the actual optimization target, and it is strongly influenced by the {\em mixing rate} of the Markov chain for Gibbs sampling, i. e. how fast the Markov chain reaches its equilibrium distribution. If the distribution in the Markov chain changes very slowly, the reconstruction error will be low, even if the distributions of the samples and the model differ much \citep{hinton_practical_2012}.

\subsubsection{Multimodal deep Boltzmann machines}

Figure \ref{fig:bmsoverview} shows different possible architectures for multimodal Boltzmann machines \citep{srivastava2012multimodal}.
Multimodal DBMs can be considered a generalization of DBMs.
They have been invented to make it possible to combine different input data types in a single model.

The training procedure of multimodal DBMs can be generalized as follows:
For the pre-training of a multimodal DBM, the different layers are trained by stacking RBMs in the same way as for pre-training a DBM.
The only differences are that the RBMs at the lowest layer may have different input distributions, and that RBMs in higher layers may be partitioned, i.e., some connections are always zero.
For the fine-tuning, the gradients in the different layers are calculated in the same way as the gradients in the respective RBM types, using the mean-field approximations and the fantasy particles of the DBM fine-tuning algorithm as activations of the respective visible and hidden nodes.
Gibbs sampling can also be performed in the same way, using the conditional probabilities in the RBMs.



\subsubsection{A new approach for modeling categorical data}\label{methodsoftmax0}
%TODO auch beschreiben, dass man bernoulli und categoriale variablen in einem haben kann
Another important statistical data type that needs to be handled when dealing with biomedical data is data with categorical values.
For most applications in machine learning, categorical data is usually encoded in dummy variables \citep{hastie_elements}.
It would be possible to use the binary dummy variables as input to a restricted or deep Boltzmann machine with Bernoulli distributed visible units as well.
But when sampling from such a Boltzmann machine model all combinations of visible nodes have a positive probability. This can be seen from the formula of the conditional probability (\ref{eqn:condprobrbm}) and the fact that the values of the sigmoid function are strictly positive.
Therefore, the resulting data is not properly encoded in general  because illegal combinations of dummy variables can occur. With that, sampled values cannot be mapped to the original categories any more.
Using dummy variables as input to Boltzmann machines with Bernoulli distributed variables makes it also more difficult to learn higher level patterns, as the Boltzmann machine has at first to learn the pattern that results from the dummy encoding  by itself. Hence it is advised to use a Boltzmann machine that has the knowledge about the encoding built into its energy function and probability distribution.


For encoding categorical variables, the most popular encoding used by the machine learning frameworks \apkg{TensorFlow} \citep{tensorflow}, \apkg{scikit-learn} \citep{scikit-learn} or \apkg{Flux} \citep{flux} is the so-called ``one-hot encoding", which encodes a variable with $k$ categories in a binary vector of $k$ components, where exactly one component is one and all others are zero.
An advantage of this is that all categories are treated equally. 
Here, I tried a slightly different variant.
A categorical variable with $k$ categories is encoded in $k-1$ binary dummy variables.
For example, this is the encoding of the values for a variable with four categories:

\begin{table}[h!]
\centering
\begin{tabular}{cc}
Categorical value & Dummy encoding \\
\hline
1 & 0 0 0 \\
2 & 1 0 0 \\
3 & 0 1 0 \\
4 & 0 0 1 \\
\end{tabular}
\caption{Dummy encoding with reference category for a categorical variable with four categories}\label{dummenc}
\end{table}

This variant is the technique for creating dummy variables for categorical variables in regression models \citep{faraway_regression}.
One category is used as reference category for the interpretation of the regression coefficients.
This allows to be parsimonious with parameters.
As already pointed out in the introduction, this may therefore help with deep learning on genetic data in particular.
Consider genetic variant data that contains values 0/1/2 for each of defined location on the human genome to encode that there is no deviation from the reference genome (0), a deviation on one chromosome (1) or a deviation on both chromosomes (2).
An RBM receiving input with dummy encoding using the natural reference category 0 needs only two thirds of the parameters that an RBM needs which receives the same data in one-hot encoding.

The energy function $E$ for RBMs with categorical data is the same as for Bernoulli distributed visible nodes (see formula \ref{eqn:energyformularbm}).
The difference is that not all combinations of visible nodes are allowed. Thus the partition function $Z = \sum_{v} \sum_{h} e^{-E(v,h)}$ and the probability
$p(v) = \frac{\sum_h e^{-E(v,h)}}{Z}$ change because the sum over all possible states of $v$ in the formula for $Z$ contains less summands than in the case with a Bernoulli distribution, where all combinations of activations are possible.

For deriving the formulas for an RBM that handles the dummy encoding like exemplified in table \ref{dummenc}, let us denote with $C_k$ the set of dummy variables that the dummy variable $k$ belongs to.
The visible nodes of the RBM can cover multiple categorical variables and multiple sets of dummy variables, which may differ in the number of categories.
If the number of categories equals two for all categorical variables, this model is the same as an RBM with Bernoulli distributed nodes.

Like \cite{krizhevsky2009tinyimagesthesis} let us write $E(v_k = 1, v_{i \neq k}, h)$ for the energy of the combination of a visible vector $v$ with $v_k = 1$ and a hidden vector $h$. Similarly, let us define $E(v_{i \in C_k} = 0, v_{i \notin C_k}, h)$ for the energy of the combination of a visible vector $v$ that is zero in all dummy variables that belong to $C_k$ and a hidden vector $h$.

From the previously defined dummy encoding results that if one dummy variable in the set is one, all others in the set are zero. 
The sum over all possible combinations of $v$ can therefore be split into those parts where a dummy variable $k$ is zero and one part where all others in the corresponding set of dummy variables are one, because these covers all allowed combinations.
This allows to split the formula for the unnormalized probability of the hidden nodes into the following sum:
\begin{align}
p^*(h) &= \sum_v  \exp (-E(v,h)) \nonumber \\
&= \sum_{v_{i \notin C_k}} \exp (-E(v_{i \in C_k} = 0, v_{i \notin C_k}, h)) + \sum_{c \in C_k} \sum_{v_{i \notin C_k}} \exp ( - E(v_c = 1, v_{i \notin C_k},  h))
%E(v_k=1, v_{i\neq k}, h) = E(v_k = 1, v_{i \notin C_k}, h).
\end{align}

The input from the hidden nodes can further be extracted from $ \sum_{v_{i \notin C_k}} e^{-E(v_{k=1}, v_{i \neq k}, h)}$ by regrouping the summands:
\begin{align}
&\sum_{v_{i \notin C_k}} \exp ( - E(v_c = 1, v_{i \notin C_k},  h)) = \nonumber \\
&\quad = \sum_{v_{i \notin C_k}} \exp \left( \sum_{i \notin C_k} v_i h_j w_{ij} + \sum_{i \notin C_k} a_i v_i + a_k  + \sum_j h_j w_{kj} +\sum_j h_j b_j \right) \nonumber \\
&\quad = \exp \left( (Wh)_k + a_k \right) \sum_{v_{i \notin C_k}} \exp \left(-E(v_{i \in C_k} = 0, v_{i \notin C_k}, h) \right)
\label{eqn:sumvksoftmax}
\end{align}

The unnormalized probability $p^*(h)$ of the hidden nodes can now be rewritten as:

\begin{align}
p^*(h) =& \sum_{v_{i \notin C_k}} \exp (-E(v_{i \in C_k} = 0, v_{i \notin C_k}, h)) \; + \nonumber \\ 
&\quad  \sum_{c \in C_k} \left( (Wh)_c + a_c \right) \sum_{v_{i \notin C_k}} \exp \left(-E(v_{i \in C_k} = 0, v_{i \notin C_k}, h) \right)
\label{eqn:unnormalizedhiddensplit}
\end{align}

With this, the conditional probability of a dummy variable being equal to one can finally be derived as follows:
\begin{align*}
p(v_k = 1 \mid h) &= \frac{p(v_k = 1, h)}{p(h)} \\
  &= \frac{p^*(v_k = 1, h)}{p^*(h)}\\
  &= \frac{\sum_{v_{i \notin C_k}} e^{-E(v_{k=1}, v_{i \neq k}, h)}}{p^*(h)} \\
 %&=  \frac{\sum_{v_{i \notin C_k}} \exp \left( \sum_{i \notin C_k} v_i h_j w_{ij} + \sum_{i \notin C_k} a_i v_i + a_k  + \sum h_j w_{kj} +\sum_j h_j b_j \right)}{p^*(h)} \\
 &\stackrel{(\ref{eqn:sumvksoftmax})}{=} \frac{\exp \left( (Wh)_k + a_k \right) \sum_{v_{i \notin C_k}} \exp \left(-E(v_{i \in C_k} = 0, v_{i \notin C_k}, h) \right)}{p^*(h)}\\
&\stackrel{(\ref{eqn:unnormalizedhiddensplit})}{=} \frac{\exp((Wh)_k + a_k)}{1 + \sum_{c \in C_k} \exp ((Wh)_c + a_c)}
  %&= \frac{\exp \left( (Wh)_k + a_k \right) \sum_{v_{i \notin C_k}} \exp \left(-E(v_{i \in C_k} = 0, v_{i \notin C_k}, h) \right)}{\sum_{u_{i\notin C_k)}
\end{align*}

\subsubsection{Generalizing annealed importance sampling for multimodal DBMs} \label{unnormalizedprobsrbm}

To generalize the evaluation of the lower bound of the likelihood in multimodal DBMs, the AIS algorithm for estimating the partition function via AIS can be generalized.
This can be done by ``summing out" or ``integrating out" the different types of visible nodes (see figure \ref{aissummingout} C).
In section \ref{methodAIS}, AIS for binary DBMs is described.
In formula (\ref{eqn:aisunnormalizedprob}) the binary visible layer is summed out by using the formula 
\[
p^*(h) = e^{b^T h} \prod_j (1+ \exp((a + W h)_j))
\]
for the unnormalized probability of the hidden units in the RBM at the input layer.
Due to the symmetry of hidden and visible nodes in a RBM with only Bernoulli distributed nodes, this formula for $p^*(h)$ can be derived analogously to the free energy $F(v) = - \log p^*(v)$. 
To get a formula like formula (\ref{eqn:aisunnormalizedprob}) for a multimodal DBM, the formulas for the unnormalized probabilities $p^*(h)$ of the different RBMs at the input layer of the multimodal DBM need to be derived.
To change the type of the input layer, these formulas can then be used in place of the first line in formula (\ref{eqn:aisunnormalizedprob}).


Since I have not found formulas for the unnormalized probability of the hidden nodes in RBMs with Gaussian visible nodes in the literature, I derive it here in detail.
The derivations for $p^*(h)$ for RBMs with Gaussian visible nodes use the fact that the integral over the density function of a normal distribution $\mathcal{N}(\mu, \sigma^2)$ is equal to one:
\begin{equation} \int \frac{1}{\sqrt{2 \pi \sigma^2}} e^{ -\frac{(x - \mu)^2}{2 \sigma^2}} = 1  
\label{eqn:densitynormal}
\end{equation}

With that, the unnormalized probability of the hidden nodes in a {\bf Gaussian RBM with original parameterization} (see formula (\ref{eqn:energyformulagbrbm})) can be calculated as

\begin{align*}
   p^*(h) &= \int e^{-E \left(v,h \right)} dv \\
   &= \int \exp \left( -\sum_{i=1}^{n_V}\frac{(v_i - a_i)^2}{2\sigma_i^2} + b^T h + \sum_{i=1}^{n_V} \sum_{j=1}^{n_H} \frac{v_i}{\sigma_i}h_j w_{ij} \right) dv\\
   &= e^{b^T h} \int \exp \left( \frac{v_i^2 -2 a_i v_i + a_i^2 - 2 v_i (Wh)_i \sigma_i}{2 \sigma_i^2} \right) dv \\
   &= e^{b^T h} \int \exp \left(
      - \sum_{i=1}^{n_V} \frac{{\left( v_i - \left( (Wh)_i \sigma_i + a_i \right) \right)}^2}{2\sigma_i^2} + \frac{1}{2}(Wh)_i^2 + (Wh)_i \frac{a_i}{\sigma_i} \right ) dv \\
   \begin{split}
      &= \exp \left(b^T h + \sum_{i=1}^{n_V} \frac{1}{2}(Wh)_i^2 + (Wh)_i \frac{a_i}{\sigma_i} \right ) \cdot \\
      & \quad \quad \int \exp \left ( - \sum_{i=1}^{n_V} \frac{{\left( v_i - ((Wh)_i \sigma_i + a_i) \right)}^2}{2\sigma_i^2} \right) dv
   \end{split} \\
   & \stackrel{(\ref{eqn:densitynormal})}{=} \exp \left( b^T h + \sum_{i=1}^{n_V} \frac{1}{2}(Wh)_i^2 + (Wh)_i \frac{a_i}{\sigma_i} \right ) \prod_{i=1}^{n_V}\left(\sqrt{2\pi} \sigma_i\right). \\
\end{align*}

For {\bf Cho's alternative parameterization} (see formula (\ref{eqn:energyformulagbrbm2})), the unnormalized probability calculates analogously as
\begin{align*}
   p^*(h) &= \int e^{-E \left(v,h \right)} dv \\
   &= \int \exp \left( - \sum_{i=1}^{n_V} \frac{(v_i - a_i)^2}{2\sigma_i^2} + \sum_{i=1}^{n_V} \sum_{j=1}^{n_H} h_j w_{ij} \frac{v_i}{\sigma_i^2} - \sum_{i=1}^{n_H} b_j h_j \right) dv \\
   &= e^{b^T h} \int \exp\left( - \sum_{i=1}^{n_V} \frac{(v_i - a_i)^2 - 2 v_i (Wh)_i}{2 \sigma_i^2} \right) dv \\
   &= e^{b^T h} \int \exp \left( - \sum_{i=1}^{n_V} \frac{\left((v_i - ((Wh)_i + a_i) \right)^2}{2 \sigma_i^2}  + \sum_{i=1}^{n_V} \frac{(Wh)_i^2 + 2 a_i (Wh)_i}{2\sigma_i^2} \right) dv\\
   &= \exp \left( b^T h + \sum_{i=1}^{n_V} \frac{(Wh)_i^2 + 2 a_i (Wh)_i}{2\sigma_i^2} \right) \cdot \\
   & \quad \quad \int \exp \left(- \sum_{i=1}^{n_V} \frac{\left((v_i - ((Wh)_i + a_i) \right)^2}{2 \sigma_i^2} \right) dv\\
   &\stackrel{(\ref{eqn:densitynormal})}{=} \exp \left( b^T h + \sum_{i=1}^{n_V} \frac{\frac{1}{2}(Wh)_i^2 + (Wh)_i a_i}{\sigma_i^2} \right ) \prod_{i=1}^{n_V}\left(\sqrt{2\pi} \sigma_i \right).
\end{align*}

For the RBM for {\bf categorical variables with the dummy encoding with reference level} from section \ref{methodsoftmax0}, the unnormalized probability can be derived similar to the RBM with Bernoulli distributed nodes. At first, the energy function can be rewritten in the same way as with the RBM with Bernoulli distributed nodes:
\begin{align}
e^{-E(v,h)} &= \exp \left(\sum_j b_j h_j + \sum_{i,j} w_{ij} v_i h_j + \sum_i a_i v_i \right) \nonumber \\
&= \exp \left( \sum_i b_j h_j + \sum_i v_i \left( a_i + \sum_j w_{ij} h_j \right) \right) \nonumber \\
&= e^{\sum b_h h_j} \prod_i \underbrace{e^{v_i (a_i + \sum_j w_{ij} h_j)}}_{(*)}
\label{eqn:freeenergytrick}
\end{align}
\begin{equation*}
(*) = \left\{
\begin{array}{l}
 =1 \text{ for } v_i = 0 \\
 = e^{a_i +\sum_j w_{ij} h_j} \text{ for } v_i = 1 
\end{array} \right.
\end{equation*}

When multiplying out the product below using equation (\ref{eqn:freeenergytrick}), it can be seen that $p^*(h)$ can be written as
\begin{align*}
p^*(h) &= \sum_v e^{-E(v,h)} \\
&= e^{\sum_j b_j h_j} \prod_{C} \left( 1 + \sum_{i \in C} e^{\sum_j w_{ij} h_j + a_i} \right).
\end{align*}
Here $C$ denotes the set of all index sets of the dummy variables, where each index set belongs to a categorical variable.

\subsection{The \apkg{BoltzmannMachines} Julia package}
TODO Einleitung zur Implementation: kurz features abreißen.
A first product of this work on deep and restricted Boltzmann machines algorithms is a software implementation that aims at  ....
Algorithms described previously 
The Julia package \apkg{BoltzmannMachines} was designed to 
This implementationwhich has been published as open source software on GitHub.
Result: officially registered Julia package. Only package covering the DBM training.
It is also the only known published implementation of AIS on DBMs with a flexible choice of the architecture, including different types of visible nodes.
Evaluation Convenient monitoring.

\subsubsection{Feature overview and comparison with existing implementations}\label{BMfeatures}

At the time of the decision whether to start a new implementation, several software implementations for RBMs and DBNs already existed.
Yet, there was no user-friendly software solution for training and evaluating DBMs.
The original implementation of DBM training\footnote{\url{http://www.cs.toronto.edu/~rsalakhu/DBM.html}} was written in MATLAB.
It consists of a collection of scripts and it is hard to re-use because it does not encapsulate the code in functions.
Therefore, the decision for creating a new software implementation of the algorithm was made.
The new implementation was designed to be very flexible and extensible for exploring new ideas, such as partitioning of DBMs and the generalization on multimodal DBMs.
This lead to the implementation of a range of features for experimenting with restricted and deep Boltzmann machines.

This section describes the features and characteristics of the existing software implementations for training and evaluating RBMs, DBNs, and DBMs.
Table \ref{tab:FeatureOverviewBoltzmann} summarizes the informations about the existing solutions and compares them with the features of the \apkg{BoltzmannMachines} package.

\begin{table}[h!]
\centering
\begin{tabular}{p{4.4cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.25cm} p{1.25cm}}
\hline \\ [3.9ex]\\
 & \begin{rotate}{25}\apkg{BoltzmannMachines} \end{rotate} & 
 \begin{rotate}{25} \apkg{lucastheis/deepbelief} \end{rotate} & 
 \begin{rotate}{25} \apkg{Boltzmann} \end{rotate} & 
 \begin{rotate}{25} \apkg{yell/boltzmann-machines}  \end{rotate} &
 \begin{rotate}{25} \apkg{scikit-learn} \end{rotate} &
 \begin{rotate}{25} \apkg{darch} \end{rotate} \\
\hline
Compared version & 1.2 & - & 0.7 & - & 0.23 & 0.12 \\
Language & Julia & Python & Julia & Python & Python & R \\
Registered package & Yes & No & Yes & No & Yes & Yes \\ 
License & MIT & MIT & MIT & MIT & 3-BSD & GPL-3 \\ 
Bernoulli RBM & Yes & Yes & Yes  & Yes & Yes & Yes \\
Categorical input & Yes & No & No & No & No & No \\
Gaussian RBM (Hinton) & Yes & Yes & No & Yes & No & No \\
Gaussian RBM (Cho) & Yes & No & No & No & No & No \\
DBN training & Yes & Yes & Yes & Yes & No & Yes \\
DBM training & Yes & No & No & Yes & No & No \\
Multimodal DBM & Yes & No & No & No & No & No \\
Exact likelihood & Yes & No & No & No & No & No \\
AIS for RBMs & Yes & Yes & No & Yes & No & No \\
AIS for DBMs & Yes & No & No & Yes & No & No \\
\hline
\end{tabular}
\caption{\label{tab:FeatureOverviewBoltzmann} Comparison of features and basic charateristics of different software implementations of restricted and deep Boltzmann machines.}
\end{table}

The \apkg{deepbelief} package \citep{deepbelief_github} provides an object-oriented implementation of RBMs and DBNs in Python, and employs the linear algebra library \apkg{NumPy} \citep{numpy} for the matrix operations.
It provides an implementation of an estimator of the likelihood of DBNs that is described in the corresponding publication \citep{theis2011deepbelief}.
It also allows training binary RBMs and Gaussian RBMs as described by \cite{krizhevsky2009tinyimages}, and it implements the AIS algorithm for estimating the likelihood for the two types of RBMs.
It served as a reference for parts of the implementation of AIS in RBMs in the \apkg{BoltzmannMachines} package.

The Julia package \apkg{Boltzmann} implements algorithms for training RBMs and DBNs. 
The only measure for evaluating RBMs provided there\footnote{\url{https://github.com/dfdx/Boltzmann.jl/blob/0a1848a190c4cd7aa2ae3c0f6b6fa83ceac1613e/src/rbm.jl\#L169}} is the pseudo-likelihood (see \cite{goodfellow_deep_2016}, chapter 18.3).
The \apkg{Boltzmann} package also offers a Gaussian RBM (type \inlinecode{GRBM}), but the formulas for the Gaussian RBM are not documented and the implementation does not match the formulas described here in \ref{gaussianrbm}.
Like the \apkg{BoltzmannMachines} package, it is written in pure Julia.


The \apkg{yell/boltzmann-machines} package dates later than the \apkg{BoltzmannMachines} package\footnote{\apkg{yell/boltzmann-machines}: first commit May 2017, \apkg{BoltzmannMachines}: first commit September 2016, first release June 2017}.
From all the existing solutions described here, it comes closest to the \apkg{BoltzmannMachines} packages in terms of the features that are covered.
It is also the only known other publicly available implementation of AIS in DBMs.
Yet, its implementation of AIS is not completely generic and covers only DBMs with two hidden layers\footnote{\url{https://github.com/yell/boltzmann-machines/blob/93ece3497816ccf08f687e3237007544268788e7/boltzmann\_machines/dbm.py\#L902}}.

The prominent Python machine learning package \apkg{scikit-learn} can train binary restricted Boltzmann machines with the \inlinecode{BernoulliRBM} module\footnote{\url{https://scikit-learn.org/0.23/modules/generated/sklearn.neural\_network.BernoulliRBM.html}}.
Similar to the \apkg{Boltzmann} Julia package, it offers as evaluation metric the pseudo-likelihood.

The R package \apkg{darch} \citep{darch} covers a similar range of features as the \apkg{Boltzmann} package, covering the training of binary RBMs and DBNs.
\apkg{darch} has been registered in the official CRAN repository of the R language but it has been archived since January 2018.
(The status of being archived means that it failed checks in newer versions of R and that it cannot be installed directly via the base R function \inlinecode{install.packages()} any more.)
It still can be installed via GitHub\footnote{\url{https://github.com/maddin79/darch}}.


The implementations of Gaussian RBMs in \apkg{deepbelief} and \apkg{yell/boltzmann-machines} do not provide the possibility to learn the noise parameter $\sigma$ for the visible nodes.
Only the \apkg{BoltzmannMachines} implements this, using the gradients described in \ref{rbmgradients}.
However, it is very hard to learn the correct value of $\sigma$ in practical applications anyway \citep{hinton_practical_2012}, so this more of theoretical and experimental interest.

The \apkg{BoltzmannMachines} package is the only known software which allows to model categorical input variables in restricted or deep Boltzmann machines.

Another unique feature of the \apkg{BoltzmannMachines} package is the possibility to calculate the exact likelihood for RBM and DBM models.
Although the calculation is computationally not feasible for bigger models, which are used in most practical use cases, this is a fast way to evaluate toy models.
This feature is also be used to validate the AIS implementation by testing its results against the exact calculated value in smaller models in the test suite of the \apkg{BoltzmannMachines} package.
This proved to be very useful for ensuring the correctness of the AIS implementation.


The Julia packages \apkg{Boltzmann} and \apkg{BoltzmannMachines} as well as the Python packages \apkg{deepbelief} and \apkg{yell/BoltzmannMachines} are published under the MIT license \citep{mitlicense}.
This allows a very free use of the code, also for commercial purposes.
\apkg{scikit-learn} uses the 3-clause BSD license \citep{bsd3}, which is similarly permissive as the MIT license but explicitly forbids the promotion of products derived from the code using the name of the copyright holders.
The \apkg{darch} package uses the GPL-3 license \citep{gplv3}, which enforces that the code of derived software must also be provided as open source.



\subsubsection{Choice of implementation technology}

Starting from the original MATLAB implementation of \cite{hinton_reducing_2006}, the first attempt for experimenting with the algorithms was to use MATLAB.
The resulting MATLAB implementation was very slow, however, and the possibilities for making MATLAB code run faster are limited.
At some point, if the performance needs to be increased further,  it becomes necessary to switch to C for time-critical portions of the code and call the C code using the MATLAB C interface.
This complication, which similarly occurs in Python and R, is known as the ``two-language problem". 
It has been the main motivation behind the Julia programming language \citep{bezanson2017julia}.
In contrast to MATLAB or R, in Julia it is possible to gradually optimize the speed of code.
One particular optimization strategy is to minimize the amount of memory allocations by reusing allocated space as much as possible.
Avoiding the costs of memory allocations and garbage collection greatly increased the speed in many parts of the code.
Allocations for matrices can be avoided by using, e.g. the function \inlinecode{mul!} instead of the normal matrix multiplication operator (\inlinecode{*}).
This function performs matrix multiplication without allocating new space for the result.
Instead it stores the result in a pre-existing matrix that is passed as argument.
In MATLAB or R this is not possible without resorting to C because there matrices are language objects that cannot be modified.

For performing deep learning on large datasets, such as image collections from the internet, optimizing the execution speed is essential.
Popular libraries like TensorFlow \citep{abadi2016tensorflow}, Apache MXNet \citep{mxnet}, PyTorch \citep{pytorch} and Theano \citep{theano}
work with abstraction of the computations on tensors (multidimensional matrices) with so called computational graphs.
This way the computations can be performed on the CPU as well as on the GPU.
The high parallelization in GPUs can bring far greater performance while at the same time it requires a high sophistication of the code to fully exploit the advantages of GPUs and port the algorithms to their SIMD (single instruction multiple data) architecture.
In order to work with the abstraction, users must adapt this new programming style, and, e.g., define the algorithms via computational graphs.

Julia also offers the possibility to use parallelization via parallel processes very easily, This works also across different computers in a compute cluster without requiring the developer to change the code to adapt to a setting with involving multiple computers.
Processes from the same or different computers can be added to a worker pool via the function \inlinecode{addprocs} and the inter-process communication is handled by Julia across the machines.
This is particulary useful for optimizing hyperparameters.
In grid search \citep{bergstra_algorithms_2011} for example, many models can be trained completely in parallel in Julia, e.g., via using the Julia function  \inlinecode{pmap}, and the execution time of the whole search can therefore be divided by the number of processes available in the compute cluster.

Julia also offered and offers many different approaches for deep learning, namely the packages \apkg{Flux} \citep{flux}, \apkg{Knet} \citep{knetjl}, \apkg{Mocha} \citep{mochajl}, and \apkg{TensorFlow} \citep{tensorflowjl} among others.
At the time of starting the work on the \apkg{BoltzmannMachines} package, the other solutions were also very young, often not very stable across different platforms, and their API changed often.
This rapid development can also be seen from the fact that the Julia packages \apkg{Mocha} and \apkg{TensorFlow} are now already officially deprecated or retired.
The \apkg{BoltzmannMachines} package depends only on the API of Julia 1.0 and has no dependencies on other packages, which eliminates the main task for maintaining a package, which is to handle changes in dependencies.

Another reason for not building on existing deep learning frameworks is that the training algorithms for RBMs, DBNs and DBMs are fundamentally different from the algorithms for training many other types of neural networks.
Most neural networks used for classification as well as GANs and VAEs are trained via a backpropagation of errors \citep{backpropagation}.
The gradients for training RBMs and DBMs (see \ref{rbmgradients} and \ref{dbmtraining}), however, rely on the Gibbs sampling and mean-field algorithm.
Thus it becomes less straightforward to implement code in such a framework and there is less benefit in doing so.
 
For a scientific use, having the flexibility to change the algorithms is often more important than their execution speed because in many cases it can be better to save human time than to save computation time. 
Coming from this perspective, we used plain Julia code because we wanted to have an easy access to the code to be able to tailor the algorithms to our needs and experiment with modifications.
With respect also to the biomedical data sets of rather small sample sizes, sufficient speed could be achieved via gradual optimization of the Julia code.



\subsubsection{Package interface}

In the following, we take a closer look at how the \apkg{BoltzmannMachines} Julia package handles the different algorithms and all their different variants.
An overview over the type relations can be found in figure \ref{umlclassdiagram}.
The diagram show the relations of the types in the \apkg{BoltzmannMachines} Julia package as a UML class diagram \citep{uml}.
Although the Julia language does not have the concept of classes and visibility, the diagram is suited well to show the connections of the types with the most important associated functions, even if only a subset of the expressiveness of UML is needed.
Instead of using classical object oriented inheritance such as in Java or C++, inheritance relationships (triangle-shaped arrows in the UML diagram) can be implemented in Julia using {\em multiple dispatch} of functions.
Multiple dispatch is one of the main design features of Julia.
It is the ability to specialize functions for all combinations of input arguments \citep{zappa_nardelli_julia_2018}.
This is similar to function overloading in classical object oriented programming, such as in Java \citep{arnold2005java} or C++ \citep{cppstandard}, but it offers a cleaner approach.
The approach of Julia allows more flexibility and is more transparent, as the dispatching in Julia happens dynamically at run-time.
So always the actual run-time type of objects is used to determine the method instead of the compile-time type.

\afterpage{%
\begin{landscape}
\begin{figure}
   \centering
   \includegraphics[scale=0.5,trim={5.2cm 4cm 6.8cm 3.4cm},clip,angle=-90]{images/BoltzmannMachinesDiagram.pdf}
   \caption{Overview of the type relationships and functions in the \apkg{Boltzmann machines} Julia package as a UML class diagram 
%   For more details on these functions, see also tables \ref{juliaFunTableTrain} and \ref{juliaFunTableEval}.
   }
   \label{umlclassdiagram}
\end{figure}
\end{landscape}
}

All types of Boltzmann machines in the package have the common abstract supertype \inlinecode{AbstractBM}.
The functions that are available for all different subtypes can be seen in figure \ref{umlclassdiagram} in the boxes below the type names.
For each of the models of Boltzmann machines, it is possible to generate samples by using Gibbs sampling via the \inlinecode{gibbssample!} function.
Conditional sampling is conveniently possible via the function \inlinecode{gibbssamplecond!} (see p.\ \pageref{bms_gibbssample!}).
Functions for evaluating the likelihood are also available for all implemented Boltzmann models.
For efficiently evaluating very small models, the likelihood and the partition function can be calculated exactly. This is also used for testing AIS by comparing the values estimated in small models to the exact ones.

The training is handled separately for DBMs and RBMs via the functions \inlinecode{fitdbm} and \inlinecode{fitrbm}, which can be used for fitting an RBM or DBM model, respectively. 
These functions get as input a data set and all hyperparameters for training the model and return the model as output.
Table \ref{juliaFunTableTrain} gives an overview of all functions that can be used for training RBMs and DBMs.


\rowcolors{1}{gray!25}{white} % alternating row colors in tables
\begin{table}[h]
\begin{tabularx}{\textwidth}{X}
   \hline
   \inlinecode{fitrbm(x;...)} \rightpageref{bms_fitrbm} \\
   Fits a restricted Boltzmann machine model for a given data set \inlinecode{x} and additional model hyperparameters \\
   \inlinecode{stackrbms(x;...)} \rightpageref{bms_stackrbms}\\
   Pre-trains a stack of RBMs on a given data set \inlinecode{x}. It can either be used for pre-training of a DBM or to train a deep belief network. \\
   \inlinecode{traindbm!(dbm, x; ...)} \rightpageref{bms_traindbm!} \\
   Fine-tunes a DBM on the data set \inlinecode{x} using the mean-field approximation procedure \\
     \inlinecode{fitdbm(x; ...)} \rightpageref{bms_fitdbm} \\
   Fits a (multimodal) deep Boltzmann machine model for a given data set \inlinecode{x} and model hyperparameters \\
   \hline
\end{tabularx}
\caption{Functions in the Julia package for training RBMs and DBMs. All these functions return the trained model. The number of training epochs can be specified via the named argument \inlinecode{epochs}. There are many more hyperparameters which can be specified. For a more detailed description of the arguments, see the page references to the package documentation, which can be found in the appendix.}\label{juliaFunTableTrain}
\end{table}

Pre-training and fine-tuning of a DBM can either be done in one step via \inlinecode{fitdbm}, or in two separate steps.
Pre-training of DBMs or training of DBNs (see \ref{dbns}), can be performed via \inlinecode{stackrbms}.
DBMs  can be then be fine-tuned (see \ref{dbmtraining}) with the function \inlinecode{traindbm!}.
The fine-tuning algorithm is based on the mean-field approximation, which is calculated via the \inlinecode{meanfield} function.
The \inlinecode{traindbm!} function operates on a (pre-trained) DBM and modifies its weights.
The reason why the function name ends with an exclamation mark is that this is a Julia convention for functions that mutate their arguments.

The lower bound of the likelihood, which is the optimization objective of the DBM training algorithm, can be calculated via \inlinecode{logproblowerbound}.
This function is available for (multimodal) DBMs in addition to the ones listed for \code{AbstractBM}s.
An overview of the functions for evaluating the likelihood in RBMs and DBMs is given in table \ref{juliaFunTableEval}.


\rowcolors{1}{gray!25}{white}
\begin{table}[h]
   \begin{tabularx}{\textwidth}{X}
   \hline
   \inlinecode{logpartitionfunction(bm; ... )} \rightpageref{bms_logpartitionfunction}\\
   Estimates log of the partition function of the Boltzmann machine \inlinecode{bm} using AIS. Additional hyperparameters for AIS may be provided. \\
   \makecell[tl]{
      \inlinecode{loglikelihood(rbm, x)} \\
      \inlinecode{loglikelihood(rbm, x, logz)}
   } \rightpageref{bms_loglikelihood} \\
   Calculates the log-likelihood of data \inlinecode{x} in the restricted Boltzmann machine model \inlinecode{rbm}. The log of the partition function can be provided as parameter \inlinecode{logz} or is estimated using AIS. \\
   \makecell[tl]{
      \inlinecode{loglikelihood(dbm, x; ...)} \\
      \inlinecode{loglikelihood(dbm, x, logz; ...)}
   } \rightpageref{bms_loglikelihood} \\
   Estimates the log-likelihood of a (multimodal) deep Boltzmann machine using AIS for each sample. Additional hyperparameters for AIS may be provided. \\
   \makecell[tl]{ 
   	\inlinecode{logproblowerbound(dbm, x; ...)} \\
   	   	\inlinecode{logproblowerbound(dbm, x, logz; ...)} \\
   	} \rightpageref{bms_logproblowerbound} \\
   Estimates the stochastic lower bound of the log-likelihood that is optimized by the training algorithm with mean-field optimization. \\
   \inlinecode{exactloglikelihood(bm, x)} \rightpageref{bms_exactloglikelihood} \\
   Calculates the log-likelihood of a deep Boltzmann machine. \\
   \inlinecode{exactlogpartitionfunction(bm, x)} \rightpageref{bms_exactlogpartitionfunction} \\
   Calculates the log of the partition function for a restricted Boltzmann machine. \\
   \hline
\end{tabularx}
\caption{Functions in the Julia package for evaluating the likelihood of RBMs and DBMs. The partition function is estimated via AIS unless a value for the parameter \inlinecode{logz} is provided. This can also be calculated exactly but this is only feasible for very small models. (For the algorithmic complexity of the corresponding algorithms, see \ref{methodExactloglik}.) }
\label{juliaFunTableEval}
\end{table}


(Multimodal) DBMs are modeled as arrays (i.e.\ one-dimensional vectors) of RBMs.
In the UML diagram this composition of DBMs from vectors of RBMs is indicated by the UML composition relationship, which is depicted as line with filled diamond at the end.
Using RBMs as building blocks allows a very flexible composition of DBMs.
Figure \ref{mdbmimplasstack} shows how RBMs and \inlinecode{PartitionedRBM}s can be  arranged  and stacked to form \inlinecode{MultimodalDBM}s or \inlinecode{PartitionedBernoulliDBM}s.

An optimization of the algorithms for DBMs that contain only Bernoulli distributed nodes such as the \inlinecode{BasicDBM} or the \inlinecode{PartitionedBernoulliDBM} is possible by specializing methods for these types.
For example, summing out nodes can be done more flexibly and effectively in DBMs with only Bernoulli distributed nodes (see figure \ref{aissummingout}).

\begin{figure}[h]
   \centering
   \includegraphics[scale=3.]{images/MDBMImpl.eps}
   \caption{Multimodal deep Boltzmann machines are modeled as a stack of restricted Boltzmann machines. On the left hand side an example model is depicted as a graph and on the right hand side the corresponding modelling in the Julia package is shown.
   The first/lowest and the second layer are modelled as a \inlinecode{PartitionedRBM}s (white boxes with grey borders), each containing a vector holding two \inlinecode{BernoulliRBM}s (grey filled boxes).
   The third/highest layer is simply a \inlinecode{BernoulliRBM}.
   All three \inlinecode{AbstractRBMs} in a vector form the \inlinecode{MultimodalDBM}.
   The arrows indicate the ordering of the vectors.}
\label{mdbmimplasstack}
\end{figure}

Different types of RBMs are modeled as Julia composite types, which encapsulate the model parameters.
The abstract type for RBMs is \inlinecode{AbstractRBM}.
The methods listed there are implemented for all subtypes of RBMs in the package.
The function \inlinecode{samplehidden} and \inlinecode{samplevisible} are used for Gibbs sampling to sample from $p(h|v)$ and $p(v|h)$, respectively (see \ref{gibbssamplingrbm}).
The functions \inlinecode{hiddenpotential} and \inlinecode{visiblepotential} calculate the deterministic activation potential $\EX_v p(h|v)$ and $\EX_h p(v|h)$, which is used for RBM training and DBN training (see \ref{dbns}).
As recommended by \cite{hinton_practical_2012} the activation potential is used for the last update of the hidden units in Gibbs sampling (see section 3.1 there), and the probabilities are also used for the visible states in the last step of sampling in the model distribution (section 3.2 there).
Calculating the \inlinecode{visiblepotential} is also needed for sampling in DBNs as shown in figure \ref{fig:dbn}, where the activation from the top layer is passed down to the visible layer in a deterministic way.
For each of these functions for sampling and calculating the activation potential, there is also a function that is suffixed with an exclamation mark.
These variants are mostly used in the implementation, as they allow to reuse allocated space, which makes the implementation much faster.

The free energy of visible states $v$ in an RBM can be calculated via  \inlinecode{freeenergy(rbm, v)}.
This is used for calculating the likelihood, as described in equation (\ref{eqn:pRBMfreeenergy}) if a value for the partition function is given.
The free energy can also be used to compare the model fit of one RBM on different data sets, e.g., to monitor the overfitting by comparing the free energy in training and test data.
The \inlinecode{reconstructionerror} in RBMs (see \ref{reconstructionerror}) is also useful for monitoring, especially as a fast way to monitor the pre-training of DBMs.

The different RBMs described in \ref{rbmtypes} and \ref{methodsoftmax0} are implemented as subtypes of \inlinecode{AbstractXBernoulliRBM}.
This abstraction is useful for exploiting the commonalities between RBMs with Bernoulli distributed hidden nodes, and thereby reducing code duplication.

The \inlinecode{BernoulliGaussianRBM} is an RBM with Bernoulli distributed visible nodes and Gaussian hidden nodes.
This type was inspired by \cite{hinton_reducing_2006}, who use such an RBM layer with Gaussian hidden nodes in the top layer of a DBN for creating a better visualization of the dimensionality reduction.

The \inlinecode{PartitionedRBM} is used as a brace around RBMs in a layer such that a partitioned layer of RBMs can be thought of and used like any \inlinecode{AbstractRBM} in the implementations of other algorithms.



\subsubsection{Usage examples}
The \apkg{BoltzmannMachines} package has been registered in the official Julia package repository. It can be installed via the following Julia code:

\begin{lstlisting}
import Pkg
Pkg.add("BoltzmannMachines")
\end{lstlisting}

The functions in the package expect the input as a matrix that contains the samples in the rows and the variables in the columns.
For the following examples, we assume that \inlinecode{x} is such a matrix of type \inlinecode{Array\{Float64,2\}} containing values of 0.0 and 1.0 with \inlinecode{nsamples} rows and \inlinecode{nvariables} columns.

\begin{lstlisting}
nsamples, nvariables = size(x)
\end{lstlisting}

After loading the package via a \inlinecode{using} statement, training an RBM or a a DBM is as simple as calling \inlinecode{fitrbm} or \inlinecode{fitdbm}, respectively.

\begin{lstlisting}
using BoltzmannMachines
rbm = fitrbm(x)
dbm = fitdbm(x)
\end{lstlisting}

But as hyperparameter tuning is very important, and the choice of the hyper parameters highly depends on the data set, using the default parameters will most likely not get an optimal result.
The most important hyperparameters for RBMs and DBMs are the number of hidden nodes, the number of epochs and the learning rate.

\begin{lstlisting}
rbm = fitrbm(x; nhidden = 50, epochs = 200, 
                learningrate = 0.001)
dbm = fitdbm(x; nhiddens = [50, 20], epochs = 200, 
                learningrate = 0.001)
\end{lstlisting}




The hyperparameters for layerwise pre-training can be defined for the whole network or for each layer separately. TODO Trainlayer, AbstractTrainLayer.


TODO normal sampling


Conditional sampling, here e. g. with the first and third variable being set 1.0, and the second variable set to 0.0, can be done via:

\begin{lstlisting}
samples(dbm, 500; conditions = [1 => 1.0; 2 => 0.0; 3 => 1.0])
\end{lstlisting}

TODO monitoring

The function \inlinecode{splitdata} (see p. \pageref{bms_splitdata}), can be used to conveniently split the data set into a training and a test data.
Here we use 80\%  of the original data set \inlinecode{x} as training data and 20\% as test data.

\begin{lstlisting}
xtrain, xtest = splitdata(x, 0.2)
\end{lstlisting}


An overview of the most important functions that can be used for monitoring can be found in table \ref{monfun}.

\rowcolors{1}{gray!25}{white}
\begin{table}[h]
   \begin{tabularx}{\textwidth}{X}
   \hline
   \inlinecode{monitorreconstructionerror!(monitor, rbm, epoch, datadict)} \rightpageref{bms_monitorreconstructionerror!}\\
     Monitors the reconstruction error in an RBM \\
     \inlinecode{monitorfreeenergy!(monitor, rbm, epoch, datadict)} \rightpageref{bms_monitorfreeenergy!}\\ 
Monitors the free energy in an RBM \\

     \inlinecode{monitorloglikelihood!(monitor, rbm, epoch, datadict; ...)} \rightpageref{bms_monitorloglikelihood!}\\ 
Monitors the (estimated) log-likelihood in an RBM \\
        \inlinecode{monitorexactloglikelihood!(monitor, bm, epoch, datadict)} \rightpageref{bms_monitorexactloglikelihood!}\\ 
Monitors the exact log-likelihood in an RBM or DBM \\
     \inlinecode{monitorlogproblowerbound!(monitor, dbm, epoch, datadict; ...)} \rightpageref{bms_monitorlogproblowerbound!}\\
  Monitors the (estimated) variational lower bound of the log likelihood in a DBM \\
   \hline
\end{tabularx}
\caption{Monitoring functions in the \apkg{BoltzmannMachines} Julia package. TODO args,  AIS params}
\label{monfun}
\end{table}






%\subsubsection{Implementation of annealed importance sampling}

%TODO cross validation






\FloatBarrier
\clearpage
\section{Connecting R and Julia with the \apkg{JuliaConnectoR}}

\subsection{Feature overview and comparison to existing solutions}
TODO feature matrix aus JuliaConnectoR-Paper

\subsection{Using \apkg{BoltzmannMachines} in R via the \apkg{JuliaConnector}}
As mentioned in section \ref{BMfeatures}, there is currently no R package that implements deep Boltzmann machines.
But via the \apkg{JuliaConnectoR}, all features of the \apkg{BoltzmannMachines} package become conveniently usable in R.
This can be demonstrated by some examples, which show how easy it becomes to translate Julia code for training and evaluating deep and restricted Boltzmann machines to R code that calls the \apkg{BoltzmannMachines} Julia package via the \apkg{JuliaConnectoR} behind the scenes.

\subsection{Implementation details}
\begin{table}
\begin{tabular}{lcl}
\hiderowcolors
\input{includes/ebnftable.tex}
\end{tabular}
\caption{Serialization grammar for the \apkg{JuliaConnectoR}. TODO: Tabelle aufteilen und erklären}
\end{table}

TODO EBNF syntax \citep{ebnf} für die Grammatik erklären

The grammar is inspired by BSON \citep{bsonspec}.


%* A string is preceded by the number of bytes to UTF-8-encode the string.
%* The sequence of unnamed/positional elements in a list is preceded by
%  the number of (named) elements that follow.
%* Standard output (after '0x50') or standard error output (after '0x5e')
%  is preceded by the number of bytes that are sent.



\FloatBarrier
\section{Deep generative models in DataSHIELD}



\subsection{Theory}

\subsubsection{Privacy concepts}
TODO $\epsilon$ differential privacy, aggregated data, membership privacy, k-anonymity, l-diversity

\subsubsection{The DataSHIELD framework}

TODO Allgemeines zu DataSHIELD

\subsubsection{Different generative approaches}
TODO VAE, GAN, MICE, Verweis Goncalves

\subsubsection{Performance measures}
Performance measures
TODO correlation, ORs, clustering approaches, visual inspection of samples

\subsubsection{Disclosure metrics}
Overfitting, membership attack

\subsection{An experiment for comparing different distributed generative approaches }


\subsubsection{Experimental setup}


\subsubsection{Results}

\subsection{The \apkg{dsBoltzmannMachines} DataSHIELD package}



%TODO Bild PCA Plot
% TODO Workflow with monitored training


\begin{figure}[h]
   \centering
   \includegraphics[scale=0.5]{images/dsBoltzmannMachinesOverview.pdf}
   \caption{Overview of dependencies between the developed packages}
 \end{figure}
 
 
\begin{figure}[h]
   \centering
   \includegraphics[scale=0.4,trim={0.1cm 0 0 0.1cm},clip]{images/dsBoltzmannLearningcurves.png}
   \caption{Examples for learning curves produced by the package \apkg{BoltzmannMachinesPlots}. This shows learning curves for the training of a DBM with three layers. The plots showing the mean reconstruction error are the result of monitoring the greedy layerwise pre-training. The plot showing the lower bound of the log probability comes is the monitoring output of the fine-tuning of the DBM. There are two curves in each plot, one for the training data and one for the test data.}
 \end{figure}

\begin{figure}[h]
   \centering
   \includegraphics[scale=0.5]{images/dsBoltzmannWorkflow.pdf}
   \caption{Exemplary workflow for using the \apkg{dsBoltzmannMachines} package}
 \end{figure}
 
 
\begin{figure}[h]
   \centering
   \includegraphics[scale=.6]{images/trainingtimes.pdf}
   \caption{Training times of DBMs using the dsBoltzmannMachines package. On the left, the DBMs are trained on data sets with 50 variables and a varying number of samples. The DBMs have two hidden layers with 50 and 10 nodes, respectively. On the right, the number of samples is fixed as 2500. The DBMs there are trained using data sets with varying numbers of variables. The hidden layers are scaled proportionally. In all cases the training consists of 30 epochs pre-training and 30 epochs fine-tuning and the monitoring is performed on the training data set and a test data set with 10 percent of the samples in the training data set.}
   \label{fig:dbmtrainingtimes}
 \end{figure}
 
\clearpage
\FloatBarrier

\section{Discussion}

\subsection{Comparison of DBMs and other generative approaches}

TODO DBMs are good at conditional sampling:

An example for using conditional sampling in a setting with biomedical data could be the simulation of a patient cohort with a specific rare disease pattern using a Boltzmann machine that has been trained with a data set of diagnoses from a more general population cohort.
A result from such a simulation could be used for a sample size calculation when planning a study involving the specific disease pattern and other combinations of characteristics.
%TODO martin paper zitieren?
In cases where certain combinations might be quite rare, the learned correlations between all the variables can help in estimating the frequencies of the combinations of specific characteristics.

\subsection{Directions in privacy-perserving deep learning}
TODO diskutieren: privacy-accounting in DBMs and other approaches

\subsection{Modeling continously valued data}

\subsection{Ideas for future work}
TODO idee modelle kombinieren, dbms verbessern missing values, softmax evaluieren


One possibility for future steps for developing the \apkg{BoltzmannMachines} package is to further optimize its performance.
In Julia version 1.5, multi-threading is now an official and stable language feature.
It could be investigated whether the use of threads could speed up some parts of the code.
For this it must be considered that the multiplication of large matrices is already multi-threaded since Julia uses the OpenBLAS library, which already uses multi-threading internally \citep{openblas}.
So performance gains may only be expected if the matrix operations cannot exhaust the threading capabilities of the CPU.

TODO discuss GPU




\subsection{Conclusion}

\clearpage
\appendix



\begin{appendices}
\singlespacing
\section{References}

\renewcommand{\bibsection}{} % funzt nur mit natbib: https://tex.stackexchange.com/a/370784/206178

\bibliography{references}

\clearpage
\onehalfspacing
\section[Publications and declaration of own contributions]{Publications and declaration of \\ own contributions}
Parts of this work have been published in the following articles:

Boltzmann arxiv

JuliaConnectoR

dsBoltzmannMachines

The contributions to the published packages and corresponding publications are as follows:

TODO

The package \apkg{BoltzmannMachines} has furthermore been used for the following articles:

Martin

Moritz Bioinformatics x2

Parts of this work have been presented in the following conferences
GMDS 2018
GMDS 2019
JuliaCon 2018
JuliaCon 2020
eRum 2020

\setlength{\emergencystretch}{3em}


\clearpage
\section[Documentation of Julia package \apkg{BoltzmannMachines}]{Documentation of Julia package \\ \apkg{BoltzmannMachines}}

This appendix lists the documentation for all exported and documented items of the \apkg{BoltzmannMachines} Julia package, as of version 1.2.0. The code of the package and more documentation can be found on the GitHub repository \url{https://github.com/stefan-m-lenz/BoltzmannMachines.jl}.

\input{appendix/BoltzmannMachines_Doku}
\clearpage
\section[Documentation of R package \apkg{JuliaConnectoR}]{Documentation of R package \\ \apkg{JuliaConnectoR}}
This appendix lists the documentation for all exported and items of the \apkg{JuliaConnectoR} R package, as of version 0.6.1. The code of the package and more documentation can be found on the GitHub repository \url{https://github.com/stefan-m-lenz/JuliaConnectoR}. The package is also available from the official R CRAN repository: \url{https://CRAN.R-project.org/package=JuliaConnectoR}.

\input{appendix/JuliaConnectoR_Doku}

\clearpage
\section[Documentation of R package \apkg{dsBoltzmannMachinesClient}]{Documentation of R package \\ \apkg{dsBoltzmannMachinesClient}}
This appendix lists the documentation for all exported items of the \apkg{dsBoltzmannMachinesClient} package, version 1.0.0. %TODO 1.0.1
The code of the package and more documentation can be found on the GitHub repository\\ \url{https://github.com/stefan-m-lenz/dsBoltzmannMachinesClient}.

This is only the client side part of the software for training deep Boltzmann machines in DataSHIELD. The client side contains the user interface. The server side part is contained in the separate R package \\
\url{https://github.com/stefan-m-lenz/dsBoltzmannMachines}.

\input{appendix/dsBoltzmannMachinesClient_Doku}

\clearpage
\section{Zusammenfassung (deutsch)}
TODO, 1 Seite

\clearpage
\section{Eidesstattliche Versicherung}
Bei der eingereichten Dissertation handelt es sich um meine eigenständig erbrachte Leistung.
Ich habe nur die angegebenen Quellen und Hilfsmittel benutzt und mich keiner unzulässigen Hilfe Dritter bedient. Insbesondere habe ich wörtlich oder sinngemäß aus anderen Werken übernommene Inhalte als solche kenntlich gemacht. 
Die Dissertation oder Teile davon habe ich bislang nicht an einer Hochschule des In- oder Auslands als Bestandteil einer Prüfungs- oder Qualifikationsleistung vorgelegt.

Die Richtigkeit der vorstehenden Erklärungen bestätige ich. Die Bedeutung der eidesstattlichen Versicherung und die strafrechtlichen Folgen einer unrichtigen oder unvollständigen eidesstattlichen Versicherung sind mir bekannt.
Ich versichere an Eides statt, dass ich nach bestem Wissen die reine Wahrheit erklärt und nichts verschwiegen habe.
\vspace{2cm}

Freiburg, den TODO \hspace{3cm} Stefan Lenz

\clearpage
\section{Danksagung}



\end{appendices}


\end{document}
